

experiment:
  username: aoife-henry
  project_name: wind_forecasting_awaken_pred510
  run_name: kestrel_awaken_pred510
  log_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/logging/
  project_root: /Users/ahenry/Documents/toolboxes/wind_forecasting
  notes: "Tuning AWAKEN with Informer+"
  extra_tags: ["awaken"] # By default already has GPU id, model and mode.

logging:
  entity: jmb0507-cu-boulder # Wandb entity (group name)
  wandb_mode: online
  save_code: false # Save code to wandb
  wandb_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/logging/wandb # WandB will create a 'wandb' subdirectory here automatically
  optuna_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/logging/optuna/
  checkpoint_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/logging/checkpoints/
  slurm_output_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/logging/slurm_logs/

optuna:
  save_trial_code: false      # Default false
  n_trials_per_worker: 10     # Test a total of 4*12=48 trials
  max_epochs: 3              # epochs per trial
  limit_train_batches: 100    # batches per epoch
  metric: "val_loss" # The metric to optimize
  direction: "minimize"       # Whether to minimize or maximize the metric
  
  resample_freq_choices: [15, 30, 45, 60] # Resample frequency in seconds

  # Pruning configuration

optuna:
  save_trial_code: false      # Default false
  n_trials_per_worker: 100     # Test a total of 4*12=48 trials
  total_study_trials:
  max_epochs: 20              # epochs per trial
  sampler: "random"           # Options: "random" (ExpectedNumInstanceSampler), "sequential" (SequentialSampler)
  sampler_params:
    tpe:
      n_startup_trials: 16
      multivariate: true
      constant_liar: true
      group: false
  base_limit_train_batches: 5000    # base batches per epoch - will be scaled based on batch_size ratio
  metric: "val_loss"          # The metric to optimize (NLL)
  direction: "minimize"  

  resample_freq_choices: [15, 30, 45, 60] # Resample frequency in seconds
  
  # Pruning configuration
  pruning:
    enabled: true
    type: "successivehalving"
    min_resource: 12 # Applies to SuccessiveHalving and Hyperband
    reduction_factor: 2
    min_early_stopping_rate: 0 # Applies to SuccessiveHalving and Hyperband
    bootstrap_count: 8 # Number of trials to complete before pruning
    # type: "hyperband"
      # type: "successivehalving"
      # min_resource: 8             # Applies to SuccessiveHalving and Hyperband
      # max_resource: 10           # Applies to Hyperband only
      # reduction_factor: 2         # Applies to SuccessiveHalving and Hyperband
      # bootstrap_count: 0          # Number of trials to complete before pruning

  # Optuna visualization configuration (additional to wandb)
  visualization:
    enabled: true
    output_dir: ${logging.optuna_dir}/visualizations
    plots:
      optimization_history: true
      parameter_importance: true
      slice_plot: true
      
  # Storage backend configuration
  storage:
    backend: "mysql"  # Options: "postgresql", "sqlite", "mysql", "journal"
    # --- SQLite Specific Settings (not used if backend is postgresql) ---
    # storage_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/logging/optuna/
    sqlite_wal: true  # Enable WAL mode for SQLite
    sqlite_timeout: 600 # Timeout in seconds for SQLite locks

    # --- MySQL Specific Settings --- #
    # db_host: tlv51ahenry01.plv10telauth03.nrel.gov
    # db_port: 3306
    # db_user: ahenry
    db_host: localhost
    db_port: 3306
    db_user: root

    # --- Optional TCP/IP Settings (if use_socket is false) ---
    # use_tcp: false
    # db_host: "localhost"
    # db_port: 5432

    # --- Optional Command Execution Settings ---
    # run_cmd_shell: false # Set to true if specific commands require shell=True

  # Optuna Dashboard auto-launch configuration
  dashboard:
    enabled: true             # Set to true to automatically launch the dashboard on rank 0
    port: 8088                # Port for the dashboard web server
    log_file: "${logging.optuna_dir}/optuna_dashboard.log" # Log file for the dashboard process


dataset: 
    # data_path: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data/preprocessed_awaken_data/short_loaded_data_normalized.parquet
    # normalization_consts_path: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data/preprocessed_awaken_data/short_loaded_data_normalization_consts.csv
    data_path: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data/awaken_data/awaken_processed_normalized.parquet
    normalization_consts_path: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data/awaken_data/awaken_processed_normalization_consts.csv
    context_length: 1020 # in seconds
    prediction_length: 510 # in seconds
    target_turbine_ids:  # or leave blank to capture all
    normalize: True 
    batch_size: 128
    base_batch_size: 128  # Base batch size for scaling calculations
    workers: 12
    overfit: False
    test_split: 0.10
    val_split: 0.10
    resample_freq: 30s
    n_splits: 1 # how many divisions of each continuity group to make which is further subdivided into training test and validation data
    per_turbine_target: True
    context_length_factor: 2
    use_pytorch_dataloader: true

model:
  distr_output: 
    class: LowRankMultivariateNormalOutput
    kwargs:
      rank: 8
  
    ############# MODEL-SPECIFIC PARAMETERS ##############
  # [ TACTiS-2 ]
  tactis:
    # General TACTiS settings
    initial_stage: 1
    stage2_start_epoch: 10
    input_encoding_normalization: true
    loss_normalization: "std" # Options: "series", "timesteps", "both", "none"
    encoder_type: "standard" # Options: "standard", "temporal"
    bagging_size: null # Optional, defaults to None in estimator
    num_parallel_samples: 100 # Num of parallel samples for probabilistic prediction
    # Marginal CDF Encoder
    marginal_embedding_dim_per_head: 8
    marginal_num_heads: 5
    marginal_num_layers: 4
    flow_input_encoder_layers: 6
    flow_series_embedding_dim: 5
    # Attentional Copula Encoder
    copula_embedding_dim_per_head: 8
    copula_num_heads: 5
    copula_num_layers: 2
    copula_input_encoder_layers: 1
    copula_series_embedding_dim: 48
    # Decoder
    decoder_dsf_num_layers: 2
    decoder_dsf_hidden_dim: 256
    decoder_mlp_num_layers: 3
    decoder_mlp_hidden_dim: 16
    decoder_transformer_num_layers: 3
    decoder_transformer_embedding_dim_per_head: 16
    decoder_transformer_num_heads: 6
    decoder_num_bins: 50
    # Optimizer Params
    lr_stage1: 0.0018
    lr_stage2: 0.0007
    weight_decay_stage1: 0.0
    weight_decay_stage2: 0.0
    # Dropout & Clipping
    dropout_rate: 0.1
    gradient_clip_val_stage1: 1000.0
    gradient_clip_val_stage2: 1000.0

  informer:
    # embedding_dimension: 32 # Determines dimension of the embedding space
    # num_encoder_layers: 2 # Number of transformer blocks stacked
    # num_decoder_layers: 1 # Number of transformer blocks stacked
    # n_heads: 8 # Number of heads for spatio-temporal attention
    # d_model: 512
    # dim_feedforward: 2048
    # activation: relu
    context_length_factor: 2
    num_encoder_layers: 2
    num_decoder_layers: 2
    d_model: 128
    n_heads: 8
    factor: 5
    lr: 6.398379938945388e-05
    weight_decay: 1e-08
    dropout: 0.15697382123348755
    warmup_steps: 50000 # Warmup steps
    steps_to_decay: 50000 # T_max = (steps_per_epoch [282] * epochs_in_stage [30]) - warmup_steps
    eta_min_fraction: 0.01  # Fraction of initial LR for eta_min cosine decay
  autoformer:
    # embedding_dimension: 32 # Determines dimension of the embedding space
    num_encoder_layers: 2 # Number of transformer blocks stacked
    num_decoder_layers: 1 # Number of transformer blocks stacked
    n_heads: 8 # Number of heads for spatio-temporal attention
    dim_feedforward: 2048
    activation: gelu
    warmup_steps: 50000 # Warmup steps
    steps_to_decay: 50000 # T_max = (steps_per_epoch [282] * epochs_in_stage [30]) - warmup_steps
    eta_min_fraction: 0.01  # Fraction of initial LR for eta_min cosine decay
  spacetimeformer:
    # embedding_dimension: 32 # Determines dimension of the embedding space
    num_encoder_layers: 3 # Number of transformer blocks stacked
    num_decoder_layers: 3 # Number of transformer blocks stacked
    n_heads: 4 # Number of heads for spatio-temporal attention
    d_model: 200
    dim_feedforward: 800
    d_queries_keys: 30
    d_values: 30
    dropout_emb: 0.2
    dropout_attn_matrix: 0.0
    dropout_attn_out: 0.0
    dropout_ff: 0.3
    dropout_qkv: 0.0
    start_token_len: 0
    global_self_attn: performer
    global_cross_attn: performer
    local_self_attn: performer
    local_cross_attn: performer
    performer_redraw_interval: 100
    use_shifted_time_windows: False
    # decay_factor: 0.25
    # l2_coeff: 1e-6
    # class_loss_imp: 0.1
    pos_emb_type: abs
    embed_method: spatio-temporal
    activation: gelu
    use_given: False # value in original paper is True
    warmup_steps: 50000 # Warmup steps
    steps_to_decay: 50000 # T_max = (steps_per_epoch [282] * epochs_in_stage [30]) - warmup_steps
    eta_min_fraction: 0.01  # Fraction of initial LR for eta_min cosine decay
  svr:
    kernel: rbf
    C: 1.0
    degree: 3
    gamma: auto
    epsilon: 0.1
    cache_size: 200
    n_neighboring_turbines: 5

callbacks:
    progress_bar:
      class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        refresh_rate: 50  # every n steps
        leave: true
    # early_stopping:
    #   class_path: lightning.pytorch.callbacks.EarlyStopping
    #   init_args:
    #     check_on_train_epoch_end: False
    #     monitor: 'val_loss'
    #     patience: 100 # Number of epochs to wait before stopping (val>epochs to disable)
    #     min_delta: 0.01 # Minimum change to be considered an improvement
    #     mode: 'min'
    #     check_finite: true
    model_checkpoint:
      class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: ${logging.checkpoint_dir}
        filename: '{epoch}-{step}-{val_loss:.2f}'
        monitor: 'val_loss'
        mode: 'min'
        save_top_k: 1
        save_last: true
    lr_monitor: # Log learning rate
      class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step # Log every step/epoch
        log_momentum: false
    dead_neuron_monitor:
      enabled: false

trainer: 
    val_check_interval: 1.0
    accelerator: cpu
    devices: auto
    num_nodes: 1
    strategy: auto
    max_epochs: 100 # Maximum number of epochs to train 100
    limit_train_batches: 20 # 5000, set to null for training, 5000 for tuning
    log_every_n_steps: 1