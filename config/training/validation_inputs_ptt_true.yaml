experiment:
  username: jmb0507
  project_name: validate_tactis_flasc_3
  run_name: validation_ptt_true_best_trial_198
  project_root: /user/taed7566/Forecasting/wind-forecasting
  log_dir: /dss/work/taed7566/Forecasting_Outputs/wind-forecasting/logs/
  notes: ""
  extra_tags: ["flasc", "validation", "tcp", "farm", "ptt_true"]

logging:
  entity: jmb0507-cu-boulder
  wandb_mode: online
  save_code: false
  wandb_dir: /dss/work/taed7566/Forecasting_Outputs/wind-forecasting/logs
  optuna_dir: /dss/work/taed7566/Forecasting_Outputs/wind-forecasting/optuna
  checkpoint_dir: /dss/work/taed7566/Forecasting_Outputs/wind-forecasting/checkpoints
  slurm_output_dir: /dss/work/taed7566/Forecasting_Outputs/wind-forecasting/logs/slurm_logs

dataset:
    sampler: "sequential"
    data_path: /user/taed7566/Forecasting/wind-forecasting/examples/data/preprocessed_flasc_data/SMARTEOLE_WakeSteering_SCADA_1minData_normalized.parquet
    normalization_consts_path:  /user/taed7566/Forecasting/wind-forecasting/examples/data/preprocessed_flasc_data/SMARTEOLE_WakeSteering_SCADA_1minData_normalization_consts.csv
    context_length: 600
    prediction_length: 300
    target_turbine_ids:
    normalize: false
    batch_size: 1024
    workers: 32
    overfit: false
    test_split: 0.20
    val_split: 0.1
    resample_freq: 60s
    n_splits: 1
    per_turbine_target: true
    context_length_factor: 2

model:
  distr_output:
    class: "LowRankMultivariateNormalOutput"
    kwargs:
      rank: 5
      sigma_init: 0.2

  ############# MODEL-SPECIFIC PARAMETERS ##############
  # [ TACTiS-2 ]
  tactis:
    # General TACTiS settings
    initial_stage: 1
    stage2_start_epoch: 30
    ac_mlp_num_layers: 2
    ac_mlp_dim: 128
    stage1_activation_function: "relu"
    stage2_activation_function: "relu"
    input_encoding_normalization: true
    scaling: "std"
    loss_normalization: "both"
    encoder_type: "standard"
    bagging_size: null
    num_parallel_samples: 1000
    # Marginal CDF Encoder
    marginal_embedding_dim_per_head: 8
    marginal_num_heads: 5
    marginal_num_layers: 4
    flow_input_encoder_layers: 6
    flow_series_embedding_dim: 5
    # Attentional Copula Encoder
    copula_embedding_dim_per_head: 8
    copula_num_heads: 5
    copula_num_layers: 2
    copula_input_encoder_layers: 1
    copula_series_embedding_dim: 48
    # Decoder
    decoder_dsf_num_layers: 2
    decoder_dsf_hidden_dim: 256
    decoder_mlp_num_layers: 3
    decoder_mlp_hidden_dim: 16
    decoder_transformer_num_layers: 3
    decoder_transformer_embedding_dim_per_head: 16
    decoder_transformer_num_heads: 6
    decoder_num_bins: 50
    # Optimizer Params
    lr_stage1: 5e-6
    lr_stage2: 2e-6
    weight_decay_stage1: 1e-5
    weight_decay_stage2: 1e-5
    # Dropout & Clipping
    dropout_rate: 0.1
    ### VALUES FOR per_turbine_target = true ###
    # Learning Rate Scheduler for Stage 1
    warmup_steps_s1: 1410
    steps_to_decay_s1: 7050
    eta_min_fraction_s1: 0.01
    # Learning Rate Scheduler for Stage 2
    warmup_steps_s2: 2256
    steps_to_decay_s2: 17484
    eta_min_fraction_s2: 0.01
    ### VALUES FOR per_turbine_target = false ###
    # # Learning Rate Scheduler for Stage 1
    # warmup_steps_s1: 201
    # steps_to_decay_s1: 1007
    # eta_min_fraction_s1: 0.01
    # # Learning Rate Scheduler for Stage 2
    # warmup_steps_s2: 322
    # steps_to_decay_s2: 2497
    # eta_min_fraction_s2: 0.01

  # [ Informer ]
  informer:
    num_encoder_layers: 2
    num_decoder_layers: 1
    n_heads: 8
    d_model: 128
    dim_feedforward: 512
    activation: relu

  # [ Autoformer ]
  autoformer:
    num_encoder_layers: 2
    num_decoder_layers: 1
    n_heads: 8
    dim_feedforward: 64
    activation: gelu

  # [ Spacetimeformer ]
  spacetimeformer:
    num_encoder_layers: 3
    num_decoder_layers: 3
    n_heads: 3
    d_model: 200
    dim_feedforward: 800
    d_queries_keys: 30
    d_values: 30
    dropout_emb: 0.2
    dropout_attn_matrix: 0.0
    dropout_attn_out: 0.0
    dropout_ff: 0.3
    dropout_qkv: 0.0
    start_token_len: 0
    performer_redraw_interval: 100
    use_shifted_time_windows: false
    pos_emb_type: abs
    embed_method: spatio-temporal
    activation: gelu
########### END OF MODEL-SPECIFIC PARAMETERS ###########

callbacks:
    progress_bar:
      class_path: lightning.pytorch.callbacks.TQDMProgressBar
      init_args:
        refresh_rate: 1
        leave: true
    # early_stopping:
    #   class_path: lightning.pytorch.callbacks.EarlyStopping
    #   init_args:
    #     monitor: 'val_loss'
    #     patience: 5 # Number of epochs to wait before stopping (val>epochs to disable) - 8 good
    #     min_delta: 25.0 # Minimum change to be considered an improvement (>5.0 after observing visual fluctuations of 2-4 per epoch on an almost-perfectly visual plateau) - 5.0 good
    #     mode: 'min'
    #     check_finite: true
    model_checkpoint:
      class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: ${logging.checkpoint_dir}
        filename: '{epoch}-{step}-{val_loss:.2f}'
        monitor: 'val_loss'
        mode: 'min'
        save_top_k: 1
        save_last: true
    lr_monitor:
      class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
        log_momentum: false
        log_weight_decay : false
    dead_neuron_monitor:
      enabled: false

trainer:
    gradient_clip_val: 0.0
    accelerator: gpu
    devices: auto
    num_nodes: 1
    strategy: "ddp"
    max_epochs: 1000
    limit_train_batches:
    log_every_n_steps: 1
    deterministic: false
    benchmark: true