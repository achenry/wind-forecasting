{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# For Optuna\n",
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_contour,\n",
    "    plot_edf,\n",
    "    plot_intermediate_values,\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_slice,\n",
    "    plot_timeline\n",
    ")\n",
    "\n",
    "# Optionally enable plotly for JupyterLab\n",
    "try:\n",
    "    import plotly.io as pio\n",
    "    pio.renderers.default = \"jupyterlab\"\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not configure plotly for JupyterLab: {e}\")\n",
    "\n",
    "# Set up aesthetics for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# Optuna logging settings - reduce verbosity\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and configuration\n",
    "DB_PATH = './flasc_tactis.db'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "print(f\"Successfully connected to database at {DB_PATH}\")\n",
    "\n",
    "# Load study from database\n",
    "study_name = None  # Will be populated after querying the database\n",
    "\n",
    "# Get the study name from the database\n",
    "cursor = conn.cursor()\n",
    "try:\n",
    "    cursor.execute(\"SELECT study_name FROM studies LIMIT 1\")\n",
    "    study_name = cursor.fetchone()[0]\n",
    "    print(f\"Found study: {study_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch study name: {e}\")\n",
    "    \n",
    "# Load the study\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=f\"sqlite:///{DB_PATH}\")\n",
    "    print(f\"Successfully loaded study '{study_name}' with {len(study.trials)} trials\")\n",
    "    print(f\"Best value achieved: {study.best_value}\")\n",
    "    print(f\"Best parameters: {study.best_params}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading study: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the study\n",
    "print(f\"Study name: {study.study_name}\")\n",
    "print(f\"Study direction: {'MINIMIZE' if study.direction == optuna.study.StudyDirection.MINIMIZE else 'MAXIMIZE'}\")\n",
    "print(f\"Total number of trials: {len(study.trials)}\")\n",
    "print(f\"Number of completed trials: {len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE]))}\")\n",
    "print(f\"Number of pruned trials: {len(study.get_trials(states=[optuna.trial.TrialState.PRUNED]))}\")\n",
    "print(f\"Number of failed trials: {len(study.get_trials(states=[optuna.trial.TrialState.FAIL]))}\")\n",
    "\n",
    "# Display best trial details\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest Trial Information:\")\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(f\"  Trial number: {best_trial.number}\")\n",
    "print(f\"  Parameters:\")\n",
    "for param_name, param_value in best_trial.params.items():\n",
    "    print(f\"    {param_name}: {param_value}\")\n",
    "\n",
    "# Check if the study has user attributes\n",
    "if study.user_attrs:\n",
    "    print(\"\\nStudy User Attributes:\")\n",
    "    for key, value in study.user_attrs.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Extract parameter names to understand the hyperparameter space\n",
    "param_names = list(best_trial.params.keys())\n",
    "print(f\"\\nHyperparameter names: {param_names}\")\n",
    "\n",
    "# Get parameter importance if scikit-learn is available\n",
    "try:\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    print(\"\\nParameter importance:\")\n",
    "    for param_name, score in importance.items():\n",
    "        print(f\"  {param_name}: {score:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not calculate parameter importance: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert trials to DataFrame for easier analysis\n",
    "def trials_to_df(study):\n",
    "    \"\"\"Convert Optuna study trials to a pandas DataFrame.\"\"\"\n",
    "    # Extract trial data\n",
    "    trials_data = []\n",
    "    for trial in study.trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            trial_dict = {\n",
    "                'number': trial.number,\n",
    "                'value': trial.value,\n",
    "                'datetime_start': trial.datetime_start,\n",
    "                'datetime_complete': trial.datetime_complete,\n",
    "                'duration': (trial.datetime_complete - trial.datetime_start).total_seconds()\n",
    "            }\n",
    "            \n",
    "            # Add parameters\n",
    "            trial_dict.update(trial.params)\n",
    "            \n",
    "            # Add user attributes\n",
    "            for key, value in trial.user_attrs.items():\n",
    "                trial_dict[f\"user_attr_{key}\"] = value\n",
    "            \n",
    "            trials_data.append(trial_dict)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(trials_data)\n",
    "    \n",
    "    # Sort by trial number\n",
    "    df = df.sort_values('number')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "trials_df = trials_to_df(study)\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(f\"DataFrame shape: {trials_df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in trials_df.columns:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(trials_df.head())\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics for objective value:\")\n",
    "display(trials_df['value'].describe())\n",
    "\n",
    "# Display statistics for each hyperparameter\n",
    "print(\"\\nHyperparameter statistics:\")\n",
    "param_cols = [col for col in trials_df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "              and not col.startswith('user_attr_')]\n",
    "display(trials_df[param_cols].describe())\n",
    "\n",
    "# Save DataFrame for later use\n",
    "trials_df.to_csv('optuna_trials.csv', index=False)\n",
    "print(\"\\nSaved trials data to 'optuna_trials.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization using Optuna's built-in tools\n",
    "\n",
    "# 1. Optimization History\n",
    "print(\"## Optimization History\")\n",
    "print(\"Shows the history of the optimization process, plotting the objective value over trial numbers.\")\n",
    "fig = plot_optimization_history(study)\n",
    "fig.update_layout(\n",
    "    title=\"Optimization History\",\n",
    "    xaxis_title=\"Trial Number\",\n",
    "    yaxis_title=\"Objective Value\",\n",
    "    width=1000,\n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Parameter Importances\n",
    "print(\"\\n## Parameter Importances\")\n",
    "print(\"Shows the relative importance of each hyperparameter in influencing the objective.\")\n",
    "try:\n",
    "    fig = plot_param_importances(study)\n",
    "    fig.update_layout(\n",
    "        title=\"Parameter Importances\",\n",
    "        xaxis_title=\"Importance\",\n",
    "        yaxis_title=\"Parameter\",\n",
    "        width=1000,\n",
    "        height=600\n",
    "    )\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot parameter importances: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Parallel Coordinate Plot\n",
    "print(\"\\n## Parallel Coordinate Plot\")\n",
    "print(\"Shows the relationship between hyperparameter values and objective value.\")\n",
    "fig = plot_parallel_coordinate(study)\n",
    "fig.update_layout(\n",
    "    title=\"Parallel Coordinate Plot\",\n",
    "    width=1200,\n",
    "    height=700\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Contour Plot\n",
    "print(\"\\n## Contour Plot\")\n",
    "print(\"Shows the relationship between pairs of hyperparameters and their impact on the objective.\")\n",
    "try:\n",
    "    fig = plot_contour(study)\n",
    "    fig.update_layout(\n",
    "        title=\"Contour Plot\",\n",
    "        width=1200,\n",
    "        height=800\n",
    "    )\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not create contour plot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Slice Plot\n",
    "print(\"\\n## Slice Plot\")\n",
    "print(\"Shows the objective value as a function of each hyperparameter, while fixing others to their optimal values.\")\n",
    "fig = plot_slice(study)\n",
    "fig.update_layout(\n",
    "    title=\"Slice Plot\",\n",
    "    width=1200,\n",
    "    height=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. EDF (Empirical Distribution Function) Plot\n",
    "print(\"\\n## EDF Plot\")\n",
    "print(\"Shows the empirical distribution of the objective values.\")\n",
    "fig = plot_edf(study)\n",
    "fig.update_layout(\n",
    "    title=\"Empirical Distribution Function of Objective Values\",\n",
    "    xaxis_title=\"Objective Value\",\n",
    "    yaxis_title=\"Cumulative Probability\",\n",
    "    width=1000,\n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Timeline\n",
    "print(\"\\n## Trial Timeline\")\n",
    "print(\"Shows the execution timeline of the trials.\")\n",
    "try:\n",
    "    # This plot requires trial start/completion times\n",
    "    fig = plot_timeline(study)\n",
    "    fig.update_layout(\n",
    "        title=\"Trial Timeline\",\n",
    "        width=1200,\n",
    "        height=600\n",
    "    )\n",
    "    fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not create timeline plot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom visualizations for deeper analysis\n",
    "\n",
    "# 1. Top N best trials with parameter values\n",
    "def plot_top_n_trials(df, n=10):\n",
    "    \"\"\"Plot the top N best performing trials along with their parameter values.\"\"\"\n",
    "    # Sort by objective value (assuming minimization)\n",
    "    top_df = df.sort_values('value').head(n).copy()\n",
    "    \n",
    "    # Create a summary figure\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        row_heights=[0.3, 0.7],\n",
    "        subplot_titles=(\"Objective Values for Top Trials\", \"Parameter Values for Top Trials\")\n",
    "    )\n",
    "    \n",
    "    # Plot objective values\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=top_df['number'], \n",
    "            y=top_df['value'],\n",
    "            text=top_df['value'].round(6),\n",
    "            textposition='auto',\n",
    "            name=\"Objective Value\",\n",
    "            marker_color='darkblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Get parameter columns\n",
    "    param_cols = [col for col in df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                  and not col.startswith('user_attr_')]\n",
    "    \n",
    "    # Normalize parameter values for comparison\n",
    "    for col in param_cols:\n",
    "        if top_df[col].dtype in [np.float64, np.int64]:\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            if max_val > min_val:  # Avoid division by zero\n",
    "                top_df[f'{col}_norm'] = (top_df[col] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                top_df[f'{col}_norm'] = 0.5  # Set to middle if min=max\n",
    "    \n",
    "    # Plot normalized parameter values\n",
    "    for i, col in enumerate(param_cols):\n",
    "        if f'{col}_norm' in top_df.columns:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=top_df['number'],\n",
    "                    y=top_df[f'{col}_norm'],\n",
    "                    mode='lines+markers',\n",
    "                    name=col\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Top {n} Trials Analysis\",\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        showlegend=True,\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    \n",
    "    # Add annotations for actual parameter values\n",
    "    for i, trial in top_df.iterrows():\n",
    "        annotations = []\n",
    "        for j, col in enumerate(param_cols):\n",
    "            if f'{col}_norm' in top_df.columns:\n",
    "                annotations.append(\n",
    "                    f\"{col}: {trial[col]}\"\n",
    "                )\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            x=trial['number'],\n",
    "            y=1.05,  # Above the plot\n",
    "            text=\"<br>\".join(annotations),\n",
    "            showarrow=False,\n",
    "            xref=\"x2\",\n",
    "            yref=\"paper\",\n",
    "            align=\"center\",\n",
    "            visible=False,\n",
    "            hovertext=\"hover\"\n",
    "        )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Correlation Heatmap\n",
    "def plot_correlation_heatmap(df):\n",
    "    \"\"\"Plot correlation between parameters and objective value.\"\"\"\n",
    "    # Get numeric columns including parameters and objective\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr = df[numeric_cols].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=corr.values,\n",
    "        x=corr.columns,\n",
    "        y=corr.index,\n",
    "        colorscale='Viridis',\n",
    "        zmin=-1, zmax=1\n",
    "    ))\n",
    "    \n",
    "    # Add correlation values as text\n",
    "    annotations = []\n",
    "    for i, row in enumerate(corr.values):\n",
    "        for j, val in enumerate(row):\n",
    "            annotations.append(\n",
    "                dict(\n",
    "                    x=corr.columns[j],\n",
    "                    y=corr.index[i],\n",
    "                    text=f\"{val:.2f}\",\n",
    "                    showarrow=False,\n",
    "                    font=dict(color='white' if abs(val) > 0.5 else 'black')\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Correlation Heatmap\",\n",
    "        annotations=annotations,\n",
    "        height=800,\n",
    "        width=1000\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Parameter Pairplot with Objective\n",
    "def plot_pairplot_with_objective(df, params=None, n_samples=None):\n",
    "    \"\"\"Create a pairplot matrix of parameters colored by objective value.\"\"\"\n",
    "    if n_samples and n_samples < len(df):\n",
    "        # Sample rows if dataset is large\n",
    "        df_sample = df.sample(n_samples, random_state=42)\n",
    "    else:\n",
    "        df_sample = df.copy()\n",
    "    \n",
    "    # Select parameters to plot\n",
    "    if params is None:\n",
    "        param_cols = [col for col in df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                      and not col.startswith('user_attr_')]\n",
    "        # Limit to top 6 parameters if there are many\n",
    "        if len(param_cols) > 6:\n",
    "            try:\n",
    "                importance = optuna.importance.get_param_importances(study)\n",
    "                param_cols = list(importance.keys())[:6]\n",
    "            except:\n",
    "                param_cols = param_cols[:6]\n",
    "    else:\n",
    "        param_cols = params\n",
    "    \n",
    "    # Add objective value for coloring\n",
    "    plot_df = df_sample[param_cols + ['value']].copy()\n",
    "    \n",
    "    # Find best trial for highlighting\n",
    "    best_trial_idx = df['value'].idxmin()\n",
    "    \n",
    "    # Add a column to identify the best trial\n",
    "    df_sample['is_best'] = False\n",
    "    if best_trial_idx in df_sample.index:\n",
    "        df_sample.loc[best_trial_idx, 'is_best'] = True\n",
    "    else:\n",
    "        # If best trial isn't in the sample, add it\n",
    "        best_trial = df.loc[[best_trial_idx]].copy()\n",
    "        best_trial['is_best'] = True\n",
    "        df_sample = pd.concat([df_sample, best_trial])\n",
    "    \n",
    "    # Create a separate dataframe with only the best trial\n",
    "    best_df = df_sample[df_sample['is_best']]\n",
    "    \n",
    "    # Create pairplot figure using plotly\n",
    "    fig = px.scatter_matrix(\n",
    "        plot_df,\n",
    "        dimensions=param_cols,\n",
    "        color='value',\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        title=\"Parameter Pairplot Colored by Objective Value\"\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        width=1100\n",
    "    )\n",
    "    \n",
    "    # Instead of highlighting directly with add_trace, \n",
    "    # use a different approach to overlay the best trial\n",
    "    # Scatter matrix indices are tricky, so create a\n",
    "    # completely separate scatter_matrix for just the best point\n",
    "    \n",
    "    # Create a custom legend entry for the best trial\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[None],\n",
    "            y=[None],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color='red',\n",
    "                size=10,\n",
    "                line=dict(color='black', width=2)\n",
    "            ),\n",
    "            name='Best Trial',\n",
    "            showlegend=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update scatter points to show best trial more prominently\n",
    "    for i in range(len(fig.data)):\n",
    "        if hasattr(fig.data[i], 'marker'):\n",
    "            # Add custom marker for best trial points\n",
    "            fig.data[i].marker.size = 7\n",
    "            # Add size array with larger size for best trial\n",
    "            if 'customdata' in fig.data[i] and fig.data[i].customdata is not None:\n",
    "                # Create size array based on whether it's the best trial\n",
    "                sizes = [15 if df.loc[best_trial_idx, 'value'] == val[0] else 7 \n",
    "                         for val in fig.data[i].customdata]\n",
    "                fig.data[i].marker.size = sizes\n",
    "                \n",
    "                # Also change the color of the best trial points\n",
    "                colors = [\n",
    "                    '#FF0000' if df.loc[best_trial_idx, 'value'] == val[0] else None\n",
    "                    for val in fig.data[i].customdata\n",
    "                ]\n",
    "                # Set outlier colors\n",
    "                for j, color in enumerate(colors):\n",
    "                    if color:\n",
    "                        if not hasattr(fig.data[i].marker, 'outliercolor'):\n",
    "                            fig.data[i].marker['outliercolor'] = []\n",
    "                        while len(fig.data[i].marker['outliercolor']) <= j:\n",
    "                            fig.data[i].marker['outliercolor'].append(None)\n",
    "                        fig.data[i].marker['outliercolor'][j] = color\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Objective Value Distribution\n",
    "def plot_objective_distribution(df):\n",
    "    \"\"\"Plot the distribution of objective values.\"\"\"\n",
    "    fig = make_subplots(rows=1, cols=2, \n",
    "                        subplot_titles=(\"Histogram of Objective Values\", \"Box Plot of Objective Values\"),\n",
    "                        specs=[[{\"type\": \"xy\"}, {\"type\": \"box\"}]])\n",
    "    \n",
    "    # Histogram\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=df['value'],\n",
    "            nbinsx=30,\n",
    "            marker_color='blue',\n",
    "            opacity=0.7,\n",
    "            name=\"Distribution\"\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add vertical line for best value\n",
    "    best_value = df['value'].min()\n",
    "    fig.add_vline(\n",
    "        x=best_value,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"red\",\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add annotation for best value\n",
    "    fig.add_annotation(\n",
    "        x=best_value,\n",
    "        y=0,\n",
    "        text=f\"Best: {best_value:.6f}\",\n",
    "        showarrow=True,\n",
    "        arrowhead=1,\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Box plot\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=df['value'],\n",
    "            marker_color='blue',\n",
    "            name=\"Objective\"\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Distribution of Objective Values\",\n",
    "        height=500,\n",
    "        width=1000\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Parameter Value Distribution\n",
    "def plot_parameter_distributions(df, top_n_params=None):\n",
    "    \"\"\"Plot distribution of parameter values.\"\"\"\n",
    "    # Get parameter columns\n",
    "    param_cols = [col for col in df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                  and not col.startswith('user_attr_')]\n",
    "    \n",
    "    # Select top parameters if requested\n",
    "    if top_n_params and len(param_cols) > top_n_params:\n",
    "        try:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            param_cols = list(importance.keys())[:top_n_params]\n",
    "        except:\n",
    "            param_cols = param_cols[:top_n_params]\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_params = len(param_cols)\n",
    "    n_cols = min(3, n_params)\n",
    "    n_rows = (n_params + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=n_rows, cols=n_cols,\n",
    "        subplot_titles=param_cols\n",
    "    )\n",
    "    \n",
    "    # Add histograms for each parameter\n",
    "    for i, param in enumerate(param_cols):\n",
    "        row = i // n_cols + 1\n",
    "        col = i % n_cols + 1\n",
    "        \n",
    "        # Check if parameter is categorical\n",
    "        if df[param].dtype == 'object' or (df[param].dtype == 'int64' and len(df[param].unique()) < 10):\n",
    "            # Create bar chart for categorical parameters\n",
    "            counts = df[param].value_counts().reset_index()\n",
    "            counts.columns = ['value', 'count']\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=counts['value'],\n",
    "                    y=counts['count'],\n",
    "                    name=param\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "        else:\n",
    "            # Create histogram for numerical parameters\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=df[param],\n",
    "                    nbinsx=20,\n",
    "                    name=param\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            \n",
    "        # Add marker for value in best trial\n",
    "        best_trial_idx = df['value'].idxmin()\n",
    "        best_param_value = df.loc[best_trial_idx, param]\n",
    "        \n",
    "        # Add vertical line for best value\n",
    "        fig.add_vline(\n",
    "            x=best_param_value,\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"red\",\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Distribution of Parameter Values (Red line: value in best trial)\",\n",
    "        height=300 * n_rows,\n",
    "        width=1000,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Objective value vs. trial duration\n",
    "def plot_objective_vs_duration(df):\n",
    "    \"\"\"Plot objective value versus trial duration.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Scatter plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df['duration'],\n",
    "            y=df['value'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color=df['number'],  # Color by trial number\n",
    "                colorscale='Viridis',\n",
    "                colorbar=dict(title=\"Trial Number\"),\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            text=df['number'].apply(lambda x: f\"Trial {x}\"),\n",
    "            hovertemplate=\"<b>%{text}</b><br>Duration: %{x:.2f}s<br>Value: %{y:.6f}<extra></extra>\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Highlight best trial\n",
    "    best_trial_idx = df['value'].idxmin()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[df.loc[best_trial_idx, 'duration']],\n",
    "            y=[df.loc[best_trial_idx, 'value']],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=15,\n",
    "                color='red',\n",
    "                symbol='star',\n",
    "                line=dict(width=2, color='black')\n",
    "            ),\n",
    "            name=\"Best Trial\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add trend line\n",
    "    try:\n",
    "        z = np.polyfit(df['duration'], df['value'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(df['duration'].min(), df['duration'].max(), 100)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_trend,\n",
    "                y=p(x_trend),\n",
    "                mode='lines',\n",
    "                line=dict(color='rgba(255, 0, 0, 0.5)', width=2),\n",
    "                name=\"Trend Line\"\n",
    "            )\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Objective Value vs. Trial Duration\",\n",
    "        xaxis_title=\"Trial Duration (seconds)\",\n",
    "        yaxis_title=\"Objective Value\",\n",
    "        height=600,\n",
    "        width=1000\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Parameter value search evolution\n",
    "def plot_parameter_evolution(df, top_n_params=None):\n",
    "    \"\"\"Plot the evolution of parameter values over trials.\"\"\"\n",
    "    # Get parameter columns\n",
    "    param_cols = [col for col in df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                  and not col.startswith('user_attr_')]\n",
    "    \n",
    "    # Select top parameters if requested\n",
    "    if top_n_params and len(param_cols) > top_n_params:\n",
    "        try:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            param_cols = list(importance.keys())[:top_n_params]\n",
    "        except:\n",
    "            param_cols = param_cols[:top_n_params]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = make_subplots(\n",
    "        rows=len(param_cols), cols=1,\n",
    "        subplot_titles=param_cols,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.03\n",
    "    )\n",
    "    \n",
    "    # Sort by trial number\n",
    "    df_sorted = df.sort_values('number')\n",
    "    \n",
    "    # Add traces for each parameter\n",
    "    for i, param in enumerate(param_cols):\n",
    "        # Normalize parameter values for better visualization\n",
    "        if df_sorted[param].dtype in [np.float64, np.int64]:\n",
    "            min_val = df_sorted[param].min()\n",
    "            max_val = df_sorted[param].max()\n",
    "            if max_val > min_val:  # Avoid division by zero\n",
    "                norm_values = (df_sorted[param] - min_val) / (max_val - min_val)\n",
    "            else:\n",
    "                norm_values = np.ones(len(df_sorted))\n",
    "            \n",
    "            # Add scatter plot for parameter values\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_sorted['number'],\n",
    "                    y=norm_values,\n",
    "                    mode='lines+markers',\n",
    "                    name=param,\n",
    "                    text=df_sorted[param],\n",
    "                    hovertemplate=\"Trial %{x}<br>\" + param + \": %{text}<extra></extra>\",\n",
    "                    marker=dict(size=8)\n",
    "                ),\n",
    "                row=i+1, col=1\n",
    "            )\n",
    "            \n",
    "            # Add second y-axis with actual parameter values\n",
    "            fig.update_yaxes(\n",
    "                title_text=\"Normalized Value\",\n",
    "                row=i+1, col=1,\n",
    "                range=[0, 1]\n",
    "            )\n",
    "        else:\n",
    "            # For categorical parameters, just plot the values\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_sorted['number'],\n",
    "                    y=df_sorted[param],\n",
    "                    mode='lines+markers',\n",
    "                    name=param,\n",
    "                    marker=dict(size=8)\n",
    "                ),\n",
    "                row=i+1, col=1\n",
    "            )\n",
    "    \n",
    "    # Add objective value as color\n",
    "    for i, param in enumerate(param_cols):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_sorted['number'],\n",
    "                y=np.ones(len(df_sorted)) * 0.5,  # Middle of the plot\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=12,\n",
    "                    color=df_sorted['value'],\n",
    "                    colorscale='Viridis',\n",
    "                    colorbar=dict(title=\"Objective Value\"),\n",
    "                    showscale=i == 0,  # Show colorbar only for first parameter\n",
    "                    opacity=0.7\n",
    "                ),\n",
    "                hoverinfo='skip',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Evolution of Parameter Values Across Trials\",\n",
    "        xaxis_title=\"Trial Number\",\n",
    "        height=200 * len(param_cols),\n",
    "        width=1000,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display plots\n",
    "print(\"## Top 10 Best Trials\")\n",
    "fig = plot_top_n_trials(trials_df, n=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Correlation Heatmap\")\n",
    "fig = plot_correlation_heatmap(trials_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Parameter Pairplot\")\n",
    "fig = plot_pairplot_with_objective(trials_df, n_samples=100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Objective Value Distribution\")\n",
    "fig = plot_objective_distribution(trials_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Parameter Value Distributions\")\n",
    "fig = plot_parameter_distributions(trials_df, top_n_params=6)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Objective Value vs. Trial Duration\")\n",
    "fig = plot_objective_vs_duration(trials_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Parameter Value Evolution\")\n",
    "fig = plot_parameter_evolution(trials_df, top_n_params=6)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive analysis using ipywidgets\n",
    "\n",
    "# 1. Interactive parameter importance threshold selector\n",
    "def create_importance_threshold_widget():\n",
    "    \"\"\"Create a widget to interactively adjust the threshold for parameter importance.\"\"\"\n",
    "    # Get parameter importance\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        \n",
    "        # Create widget\n",
    "        threshold_slider = widgets.FloatSlider(\n",
    "            value=0.01,\n",
    "            min=0.0,\n",
    "            max=0.5,\n",
    "            step=0.01,\n",
    "            description='Threshold:',\n",
    "            continuous_update=False\n",
    "        )\n",
    "        \n",
    "        # Define output function\n",
    "        out = widgets.Output()\n",
    "        \n",
    "        def update_plot(change):\n",
    "            with out:\n",
    "                out.clear_output(wait=True)\n",
    "                \n",
    "                # Filter parameters by importance\n",
    "                filtered_importance = {k: v for k, v in importance.items() if v >= threshold_slider.value}\n",
    "                \n",
    "                if not filtered_importance:\n",
    "                    print(\"No parameters meet the threshold. Please lower the threshold.\")\n",
    "                    return\n",
    "                \n",
    "                # Create bar chart\n",
    "                params = list(filtered_importance.keys())\n",
    "                values = list(filtered_importance.values())\n",
    "                \n",
    "                fig = go.Figure(data=[\n",
    "                    go.Bar(\n",
    "                        x=values,\n",
    "                        y=params,\n",
    "                        orientation='h',\n",
    "                        marker_color='navy'\n",
    "                    )\n",
    "                ])\n",
    "                \n",
    "                fig.update_layout(\n",
    "                    title=f\"Parameter Importance (Threshold: {threshold_slider.value:.2f})\",\n",
    "                    xaxis_title=\"Importance Score\",\n",
    "                    yaxis_title=\"Parameter\",\n",
    "                    height=400 + len(params) * 20,\n",
    "                    width=800\n",
    "                )\n",
    "                \n",
    "                fig.show()\n",
    "        \n",
    "        # Register callback\n",
    "        threshold_slider.observe(update_plot, names='value')\n",
    "        \n",
    "        # Initialize plot\n",
    "        update_plot(None)\n",
    "        \n",
    "        # Display widget and output\n",
    "        display(widgets.VBox([threshold_slider, out]))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not create importance threshold widget: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Interactive parameter filter for trial viewer\n",
    "def create_parameter_filter_widget():\n",
    "    \"\"\"Create widgets to filter trials based on parameter values.\"\"\"\n",
    "    # Get parameter columns\n",
    "    param_cols = [col for col in trials_df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                  and not col.startswith('user_attr_')]\n",
    "    \n",
    "    # Create widgets for each parameter\n",
    "    param_widgets = {}\n",
    "    for param in param_cols:\n",
    "        # Check parameter type\n",
    "        if trials_df[param].dtype in [np.float64, np.int64]:\n",
    "            # Numerical parameter - use range slider\n",
    "            min_val = trials_df[param].min()\n",
    "            max_val = trials_df[param].max()\n",
    "            step = (max_val - min_val) / 100\n",
    "            if trials_df[param].dtype == np.int64:\n",
    "                step = max(1, int(step))\n",
    "            \n",
    "            param_widgets[param] = widgets.FloatRangeSlider(\n",
    "                value=[min_val, max_val],\n",
    "                min=min_val,\n",
    "                max=max_val,\n",
    "                step=step,\n",
    "                description=f'{param}:',\n",
    "                continuous_update=False,\n",
    "                layout=widgets.Layout(width='600px')\n",
    "            )\n",
    "        else:\n",
    "            # Categorical parameter - use multiple selection\n",
    "            options = trials_df[param].unique().tolist()\n",
    "            param_widgets[param] = widgets.SelectMultiple(\n",
    "                options=options,\n",
    "                value=options,\n",
    "                description=f'{param}:',\n",
    "                layout=widgets.Layout(width='600px', height='100px')\n",
    "            )\n",
    "    \n",
    "    # Create \"Top N trials\" selector\n",
    "    top_n = widgets.IntSlider(\n",
    "        value=10,\n",
    "        min=1,\n",
    "        max=50,\n",
    "        step=1,\n",
    "        description='Top N:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    # Create button to apply filters\n",
    "    apply_button = widgets.Button(\n",
    "        description='Apply Filters',\n",
    "        button_style='primary',\n",
    "        icon='filter'\n",
    "    )\n",
    "    \n",
    "    # Create output area\n",
    "    out = widgets.Output()\n",
    "    \n",
    "    # Define filter function\n",
    "    def apply_filters(b):\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            \n",
    "            # Start with all trials\n",
    "            filtered_df = trials_df.copy()\n",
    "            \n",
    "            # Apply parameter filters\n",
    "            for param, widget in param_widgets.items():\n",
    "                if isinstance(widget, widgets.FloatRangeSlider):\n",
    "                    min_val, max_val = widget.value\n",
    "                    filtered_df = filtered_df[(filtered_df[param] >= min_val) & (filtered_df[param] <= max_val)]\n",
    "                elif isinstance(widget, widgets.SelectMultiple):\n",
    "                    if widget.value:  # If some options are selected\n",
    "                        filtered_df = filtered_df[filtered_df[param].isin(widget.value)]\n",
    "            \n",
    "            # Display summary\n",
    "            print(f\"Filtered to {len(filtered_df)} trials out of {len(trials_df)} total trials\")\n",
    "            \n",
    "            # Display top N trials\n",
    "            top_filtered = filtered_df.sort_values('value').head(top_n.value)\n",
    "            \n",
    "            if len(top_filtered) > 0:\n",
    "                print(f\"\\nTop {len(top_filtered)} trials after filtering:\")\n",
    "                display(top_filtered[['number', 'value'] + param_cols])\n",
    "                \n",
    "                # Plot top N trials\n",
    "                fig = make_subplots(rows=1, cols=1)\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=top_filtered['number'],\n",
    "                        y=top_filtered['value'],\n",
    "                        text=top_filtered['value'].round(6),\n",
    "                        textposition='auto',\n",
    "                        marker_color='darkblue',\n",
    "                        name=\"Objective Value\"\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                fig.update_layout(\n",
    "                    title=f\"Top {len(top_filtered)} Trials After Filtering\",\n",
    "                    xaxis_title=\"Trial Number\",\n",
    "                    yaxis_title=\"Objective Value\",\n",
    "                    height=500,\n",
    "                    width=900\n",
    "                )\n",
    "                \n",
    "                fig.show()\n",
    "            else:\n",
    "                print(\"No trials match the filter criteria. Please adjust the filters.\")\n",
    "    \n",
    "    # Register callback\n",
    "    apply_button.on_click(apply_filters)\n",
    "    \n",
    "    # Create reset button\n",
    "    reset_button = widgets.Button(\n",
    "        description='Reset Filters',\n",
    "        button_style='warning',\n",
    "        icon='refresh'\n",
    "    )\n",
    "    \n",
    "    # Define reset function\n",
    "    def reset_filters(b):\n",
    "        for param, widget in param_widgets.items():\n",
    "            if isinstance(widget, widgets.FloatRangeSlider):\n",
    "                widget.value = [widget.min, widget.max]\n",
    "            elif isinstance(widget, widgets.SelectMultiple):\n",
    "                widget.value = widget.options\n",
    "        \n",
    "        # Also clear output\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            print(\"Filters reset. Click 'Apply Filters' to view all trials.\")\n",
    "    \n",
    "    # Register callback\n",
    "    reset_button.on_click(reset_filters)\n",
    "    \n",
    "    # Create widget layout\n",
    "    param_box = widgets.VBox([widgets.Label(\"Parameter Filters:\")] + \n",
    "                             [widgets.VBox([widget]) for widget in param_widgets.values()])\n",
    "    \n",
    "    control_box = widgets.HBox([top_n, apply_button, reset_button])\n",
    "    \n",
    "    # Display all widgets\n",
    "    display(widgets.VBox([param_box, control_box, out]))\n",
    "    \n",
    "    # Initial call to show data\n",
    "    apply_filters(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Interactive trial comparator\n",
    "def create_trial_comparator_widget():\n",
    "    \"\"\"Create a widget to compare two trials side by side.\"\"\"\n",
    "    # Get list of trials\n",
    "    trial_numbers = trials_df['number'].tolist()\n",
    "    \n",
    "    # Create trial selector widgets\n",
    "    trial1 = widgets.Dropdown(\n",
    "        options=trial_numbers,\n",
    "        value=trial_numbers[0],\n",
    "        description='Trial 1:',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    trial2 = widgets.Dropdown(\n",
    "        options=trial_numbers,\n",
    "        value=study.best_trial.number,\n",
    "        description='Trial 2:',\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    \n",
    "    # Create button to compare\n",
    "    compare_button = widgets.Button(\n",
    "        description='Compare',\n",
    "        button_style='primary',\n",
    "        icon='exchange'\n",
    "    )\n",
    "    \n",
    "    # Create output area\n",
    "    out = widgets.Output()\n",
    "    \n",
    "    # Define comparison function\n",
    "    def compare_trials(b):\n",
    "        with out:\n",
    "            out.clear_output(wait=True)\n",
    "            \n",
    "            # Get trials\n",
    "            t1 = trials_df[trials_df['number'] == trial1.value].iloc[0]\n",
    "            t2 = trials_df[trials_df['number'] == trial2.value].iloc[0]\n",
    "            \n",
    "            # Create comparison table\n",
    "            comparison_data = []\n",
    "            \n",
    "            # Add objective value\n",
    "            comparison_data.append({\n",
    "                'Parameter': 'Objective Value',\n",
    "                'Trial 1': t1['value'],\n",
    "                'Trial 2': t2['value'],\n",
    "                'Difference': t2['value'] - t1['value'],\n",
    "                'Percent Change': (t2['value'] - t1['value']) / abs(t1['value']) * 100 if t1['value'] != 0 else float('inf')\n",
    "            })\n",
    "            \n",
    "            # Add parameters\n",
    "            param_cols = [col for col in trials_df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                          and not col.startswith('user_attr_')]\n",
    "            \n",
    "            for param in param_cols:\n",
    "                p1 = t1[param]\n",
    "                p2 = t2[param]\n",
    "                \n",
    "                if isinstance(p1, (int, float)) and isinstance(p2, (int, float)):\n",
    "                    diff = p2 - p1\n",
    "                    pct = (p2 - p1) / abs(p1) * 100 if p1 != 0 else float('inf')\n",
    "                else:\n",
    "                    diff = \"N/A\"\n",
    "                    pct = \"N/A\"\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    'Parameter': param,\n",
    "                    'Trial 1': p1,\n",
    "                    'Trial 2': p2,\n",
    "                    'Difference': diff,\n",
    "                    'Percent Change': pct\n",
    "                })\n",
    "            \n",
    "            # Create DataFrame\n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            \n",
    "            # Display comparison\n",
    "            display(HTML(f\"<h3>Comparison between Trial {trial1.value} and Trial {trial2.value}</h3>\"))\n",
    "            display(comparison_df)\n",
    "            \n",
    "            # Create bar chart to visualize differences\n",
    "            numeric_params = [p for p in param_cols if isinstance(t1[p], (int, float)) and isinstance(t2[p], (int, float))]\n",
    "            \n",
    "            # Normalize parameter values for better visualization\n",
    "            normalized_data = []\n",
    "            for param in numeric_params:\n",
    "                # Get min/max across all trials for this parameter\n",
    "                min_val = trials_df[param].min()\n",
    "                max_val = trials_df[param].max()\n",
    "                \n",
    "                if max_val > min_val:  # Avoid division by zero\n",
    "                    norm_p1 = (t1[param] - min_val) / (max_val - min_val)\n",
    "                    norm_p2 = (t2[param] - min_val) / (max_val - min_val)\n",
    "                    \n",
    "                    normalized_data.append({\n",
    "                        'Parameter': param,\n",
    "                        'Trial 1': norm_p1,\n",
    "                        'Trial 2': norm_p2,\n",
    "                        'Original 1': t1[param],\n",
    "                        'Original 2': t2[param]\n",
    "                    })\n",
    "            \n",
    "            # Create subplot\n",
    "            fig = make_subplots(rows=1, cols=2, \n",
    "                                subplot_titles=(\"Normalized Parameter Values\", \"Objective Value\"),\n",
    "                                specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]])\n",
    "            \n",
    "            # Parameter comparison\n",
    "            norm_df = pd.DataFrame(normalized_data)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    y=norm_df['Parameter'],\n",
    "                    x=norm_df['Trial 1'],\n",
    "                    name=f'Trial {trial1.value}',\n",
    "                    orientation='h',\n",
    "                    marker_color='blue',\n",
    "                    text=norm_df['Original 1'],\n",
    "                    hovertemplate='%{y}: %{text}<extra></extra>'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    y=norm_df['Parameter'],\n",
    "                    x=norm_df['Trial 2'],\n",
    "                    name=f'Trial {trial2.value}',\n",
    "                    orientation='h',\n",
    "                    marker_color='red',\n",
    "                    text=norm_df['Original 2'],\n",
    "                    hovertemplate='%{y}: %{text}<extra></extra>'\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "            \n",
    "            # Objective value comparison\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=['Objective'],\n",
    "                    y=[t1['value']],\n",
    "                    name=f'Trial {trial1.value}',\n",
    "                    marker_color='blue'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=['Objective'],\n",
    "                    y=[t2['value']],\n",
    "                    name=f'Trial {trial2.value}',\n",
    "                    marker_color='red'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                title=f\"Comparison between Trial {trial1.value} and Trial {trial2.value}\",\n",
    "                barmode='group',\n",
    "                height=500 + len(numeric_params) * 20,\n",
    "                width=1000\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "    \n",
    "    # Register callback\n",
    "    compare_button.on_click(compare_trials)\n",
    "    \n",
    "    # Create a \"best trial\" button\n",
    "    best_button = widgets.Button(\n",
    "        description='Compare with Best',\n",
    "        button_style='success',\n",
    "        icon='star'\n",
    "    )\n",
    "    \n",
    "    # Define best trial function\n",
    "    def compare_with_best(b):\n",
    "        trial2.value = study.best_trial.number\n",
    "        compare_trials(None)\n",
    "    \n",
    "    # Register callback\n",
    "    best_button.on_click(compare_with_best)\n",
    "    \n",
    "    # Create widget layout\n",
    "    control_box = widgets.HBox([trial1, trial2, compare_button, best_button])\n",
    "    \n",
    "    # Display all widgets\n",
    "    display(widgets.VBox([control_box, out]))\n",
    "    \n",
    "    # Initial call to show data\n",
    "    compare_trials(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Parameter importance analysis\n",
    "def create_param_importance_analyzer():\n",
    "    \"\"\"Create interactive widget to analyze parameter importance.\"\"\"\n",
    "    try:\n",
    "        # Get parameter importance\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        \n",
    "        # Create selecting widget for evaluator\n",
    "        evaluator = widgets.Dropdown(\n",
    "            options=[\n",
    "                'Default', \n",
    "                'Fanova', \n",
    "                'Mean Decrease Impurity', \n",
    "                'PedAnova'\n",
    "            ],\n",
    "            value='Default',\n",
    "            description='Evaluator:',\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Create button to recalculate\n",
    "        recalc_button = widgets.Button(\n",
    "            description='Recalculate',\n",
    "            button_style='primary',\n",
    "            icon='refresh'\n",
    "        )\n",
    "        \n",
    "        # Create output area\n",
    "        out = widgets.Output()\n",
    "        \n",
    "        # Define recalculation function\n",
    "        def recalculate(b):\n",
    "            with out:\n",
    "                out.clear_output(wait=True)\n",
    "                \n",
    "                print(\"Calculating parameter importance...\")\n",
    "                \n",
    "                # Select evaluator\n",
    "                if evaluator.value == 'Default':\n",
    "                    imp_evaluator = None\n",
    "                elif evaluator.value == 'Fanova':\n",
    "                    from optuna.importance import FanovaImportanceEvaluator\n",
    "                    imp_evaluator = FanovaImportanceEvaluator()\n",
    "                elif evaluator.value == 'Mean Decrease Impurity':\n",
    "                    from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "                    imp_evaluator = MeanDecreaseImpurityImportanceEvaluator()\n",
    "                elif evaluator.value == 'PedAnova':\n",
    "                    from optuna.importance import PedAnovaImportanceEvaluator\n",
    "                    imp_evaluator = PedAnovaImportanceEvaluator()\n",
    "                \n",
    "                try:\n",
    "                    # Calculate importance\n",
    "                    if imp_evaluator:\n",
    "                        importance = optuna.importance.get_param_importances(study, evaluator=imp_evaluator)\n",
    "                    else:\n",
    "                        importance = optuna.importance.get_param_importances(study)\n",
    "                    \n",
    "                    # Create bar chart\n",
    "                    params = list(importance.keys())\n",
    "                    values = list(importance.values())\n",
    "                    \n",
    "                    fig = go.Figure(data=[\n",
    "                        go.Bar(\n",
    "                            x=values,\n",
    "                            y=params,\n",
    "                            orientation='h',\n",
    "                            marker_color='navy'\n",
    "                        )\n",
    "                    ])\n",
    "                    \n",
    "                    fig.update_layout(\n",
    "                        title=f\"Parameter Importance using {evaluator.value} Evaluator\",\n",
    "                        xaxis_title=\"Importance Score\",\n",
    "                        yaxis_title=\"Parameter\",\n",
    "                        height=400 + len(params) * 20,\n",
    "                        width=800\n",
    "                    )\n",
    "                    \n",
    "                    fig.show()\n",
    "                    \n",
    "                    # Show importance table\n",
    "                    importance_df = pd.DataFrame({\n",
    "                        'Parameter': params,\n",
    "                        'Importance': values\n",
    "                    }).sort_values('Importance', ascending=False)\n",
    "                    \n",
    "                    print(\"\\nParameter Importance Values:\")\n",
    "                    display(importance_df)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating importance: {e}\")\n",
    "        \n",
    "        # Register callback\n",
    "        recalc_button.on_click(recalculate)\n",
    "        \n",
    "        # Create widget layout\n",
    "        control_box = widgets.HBox([evaluator, recalc_button])\n",
    "        \n",
    "        # Display all widgets\n",
    "        display(widgets.VBox([control_box, out]))\n",
    "        \n",
    "        # Initial call to show data\n",
    "        recalculate(None)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not create parameter importance analyzer: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the interactive widgets\n",
    "print(\"## Interactive Parameter Importance Threshold\")\n",
    "print(\"Use the slider to adjust the threshold for displaying parameter importance.\")\n",
    "create_importance_threshold_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Interactive Trial Filter\")\n",
    "print(\"Filter trials based on parameter values to find the best performing trials within specific parameter ranges.\")\n",
    "create_parameter_filter_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Interactive Trial Comparator\")\n",
    "print(\"Compare any two trials side by side to analyze differences in parameters and performance.\")\n",
    "create_trial_comparator_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Parameter Importance Analyzer\")\n",
    "print(\"Analyze parameter importance using different evaluation techniques.\")\n",
    "create_param_importance_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Hyperparameter Analysis\n",
    "\n",
    "# 1. Learning Curves Analysis\n",
    "def analyze_learning_curves():\n",
    "    \"\"\"Analyze the learning curves of the optimization process.\"\"\"\n",
    "    # Sort trials by execution order\n",
    "    sorted_trials = sorted(study.trials, key=lambda t: t.number)\n",
    "    \n",
    "    # Extract data\n",
    "    trial_numbers = []\n",
    "    best_values = []\n",
    "    values = []\n",
    "    \n",
    "    best_so_far = float('inf')  # Assuming minimization\n",
    "    for trial in sorted_trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            trial_numbers.append(trial.number)\n",
    "            values.append(trial.value)\n",
    "            \n",
    "            # Update best value\n",
    "            if trial.value < best_so_far:\n",
    "                best_so_far = trial.value\n",
    "            \n",
    "            best_values.append(best_so_far)\n",
    "    \n",
    "    # Create plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add all trials\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trial_numbers,\n",
    "            y=values,\n",
    "            mode='markers',\n",
    "            name='Trial Values',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color='blue',\n",
    "                opacity=0.5\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add best value curve\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trial_numbers,\n",
    "            y=best_values,\n",
    "            mode='lines',\n",
    "            name='Best Value',\n",
    "            line=dict(\n",
    "                color='red',\n",
    "                width=2\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add moving average\n",
    "    window_size = min(50, len(values) // 5) if len(values) >= 10 else 2\n",
    "    if window_size >= 2:\n",
    "        moving_avg = np.convolve(values, np.ones(window_size)/window_size, mode='valid')\n",
    "        ma_x = trial_numbers[window_size-1:]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ma_x,\n",
    "                y=moving_avg,\n",
    "                mode='lines',\n",
    "                name=f'Moving Average (window={window_size})',\n",
    "                line=dict(\n",
    "                    color='green',\n",
    "                    width=2,\n",
    "                    dash='dash'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Optimization Learning Curve\",\n",
    "        xaxis_title=\"Trial Number\",\n",
    "        yaxis_title=\"Objective Value\",\n",
    "        height=600,\n",
    "        width=1000\n",
    "    )\n",
    "    \n",
    "    # Add annotations for notable points\n",
    "    # 1. Initial best point\n",
    "    if len(trial_numbers) > 0:\n",
    "        fig.add_annotation(\n",
    "            x=trial_numbers[0],\n",
    "            y=best_values[0],\n",
    "            text=\"Initial Best\",\n",
    "            showarrow=True,\n",
    "            arrowhead=1\n",
    "        )\n",
    "    \n",
    "    # 2. Final best point\n",
    "    if len(trial_numbers) > 0:\n",
    "        final_idx = len(trial_numbers) - 1\n",
    "        fig.add_annotation(\n",
    "            x=trial_numbers[final_idx],\n",
    "            y=best_values[final_idx],\n",
    "            text=\"Final Best\",\n",
    "            showarrow=True,\n",
    "            arrowhead=1\n",
    "        )\n",
    "    \n",
    "    # 3. Biggest improvement\n",
    "    if len(best_values) > 1:\n",
    "        improvements = [best_values[i-1] - best_values[i] for i in range(1, len(best_values))]\n",
    "        if improvements:\n",
    "            max_imp_idx = np.argmax(improvements) + 1\n",
    "            fig.add_annotation(\n",
    "                x=trial_numbers[max_imp_idx],\n",
    "                y=best_values[max_imp_idx],\n",
    "                text=\"Largest Improvement\",\n",
    "                showarrow=True,\n",
    "                arrowhead=1\n",
    "            )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Hyperparameter Convergence Analysis\n",
    "def analyze_hyperparameter_convergence(trials_df, param_names=None, window_size=20):\n",
    "    \"\"\"Analyze how hyperparameters converge during optimization.\"\"\"\n",
    "    if param_names is None:\n",
    "        # Try to get top parameters by importance\n",
    "        try:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            param_names = list(importance.keys())[:5]  # Top 5 parameters\n",
    "        except:\n",
    "            # Get all parameter names\n",
    "            param_names = [col for col in trials_df.columns if col not in \n",
    "                          ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                          and not col.startswith('user_attr_')]\n",
    "            param_names = param_names[:5]  # Limit to first 5\n",
    "    \n",
    "    # Sort by trial number\n",
    "    sorted_df = trials_df.sort_values('number')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = make_subplots(\n",
    "        rows=len(param_names), \n",
    "        cols=1,\n",
    "        subplot_titles=[f\"{param} Convergence\" for param in param_names],\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.05\n",
    "    )\n",
    "    \n",
    "    # Add traces for each parameter\n",
    "    for i, param in enumerate(param_names):\n",
    "        # Check if numerical parameter\n",
    "        if sorted_df[param].dtype in [np.float64, np.int64]:\n",
    "            # Calculate rolling statistics if enough trials\n",
    "            if len(sorted_df) >= window_size:\n",
    "                # Rolling mean\n",
    "                rolling_mean = sorted_df[param].rolling(window=window_size).mean()\n",
    "                \n",
    "                # Rolling standard deviation\n",
    "                rolling_std = sorted_df[param].rolling(window=window_size).std()\n",
    "                \n",
    "                # Add rolling mean\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=sorted_df['number'],\n",
    "                        y=rolling_mean,\n",
    "                        mode='lines',\n",
    "                        name=f'{param} Rolling Mean',\n",
    "                        line=dict(\n",
    "                            color='blue',\n",
    "                            width=2\n",
    "                        )\n",
    "                    ),\n",
    "                    row=i+1, col=1\n",
    "                )\n",
    "                \n",
    "                # Add confidence interval (mean  std)\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=np.concatenate([sorted_df['number'], sorted_df['number'][::-1]]),\n",
    "                        y=np.concatenate([rolling_mean + rolling_std, (rolling_mean - rolling_std)[::-1]]),\n",
    "                        fill='toself',\n",
    "                        fillcolor='rgba(0, 0, 255, 0.1)',\n",
    "                        line=dict(color='rgba(0, 0, 255, 0)'),\n",
    "                        name=f'{param}  '\n",
    "                    ),\n",
    "                    row=i+1, col=1\n",
    "                )\n",
    "            \n",
    "            # Add parameter values\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=sorted_df['number'],\n",
    "                    y=sorted_df[param],\n",
    "                    mode='markers',\n",
    "                    name=param,\n",
    "                    marker=dict(\n",
    "                        size=5,\n",
    "                        opacity=0.5,\n",
    "                        color=sorted_df['value'],\n",
    "                        colorscale='Viridis',\n",
    "                        colorbar=dict(title=\"Objective\"),\n",
    "                        showscale=i == 0  # Show colorbar only once\n",
    "                    )\n",
    "                ),\n",
    "                row=i+1, col=1\n",
    "            )\n",
    "            \n",
    "            # Add best trial marker\n",
    "            best_trial_idx = sorted_df['value'].idxmin()\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[sorted_df.loc[best_trial_idx, 'number']],\n",
    "                    y=[sorted_df.loc[best_trial_idx, param]],\n",
    "                    mode='markers',\n",
    "                    name=f'Best {param}',\n",
    "                    marker=dict(\n",
    "                        size=10,\n",
    "                        color='red',\n",
    "                        symbol='star'\n",
    "                    ),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=i+1, col=1\n",
    "            )\n",
    "        else:\n",
    "            # Categorical parameter - show frequency of each value over time\n",
    "            categories = sorted_df[param].unique()\n",
    "            \n",
    "            # Calculate frequency of each category in rolling window\n",
    "            if len(sorted_df) >= window_size:\n",
    "                for category in categories:\n",
    "                    # Calculate rolling frequency\n",
    "                    mask = (sorted_df[param] == category)\n",
    "                    rolling_freq = mask.rolling(window=window_size).mean()\n",
    "                    \n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=sorted_df['number'],\n",
    "                            y=rolling_freq,\n",
    "                            mode='lines',\n",
    "                            name=f'{param}={category}',\n",
    "                            line=dict(width=2)\n",
    "                        ),\n",
    "                        row=i+1, col=1\n",
    "                    )\n",
    "            \n",
    "            # Add markers for actual values\n",
    "            for category in categories:\n",
    "                cat_df = sorted_df[sorted_df[param] == category]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=cat_df['number'],\n",
    "                        y=np.ones(len(cat_df)) * 0.5,  # Middle of the plot\n",
    "                        mode='markers',\n",
    "                        name=f'{param}={category}',\n",
    "                        marker=dict(\n",
    "                            size=8,\n",
    "                            opacity=0.7\n",
    "                        )\n",
    "                    ),\n",
    "                    row=i+1, col=1\n",
    "                )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Hyperparameter Convergence Analysis (Window Size: {window_size})\",\n",
    "        height=300 * len(param_names),\n",
    "        width=1000,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update y-axis titles\n",
    "    for i, param in enumerate(param_names):\n",
    "        fig.update_yaxes(title_text=param, row=i+1, col=1)\n",
    "    \n",
    "    # Update x-axis title (only for the last subplot)\n",
    "    fig.update_xaxes(title_text=\"Trial Number\", row=len(param_names), col=1)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Parallel Coordinates with Custom Highlighting\n",
    "def plot_enhanced_parallel_coordinate(trials_df, highlight_n=10):\n",
    "    \"\"\"Create an enhanced parallel coordinates plot with highlighting of best trials.\"\"\"\n",
    "    # Get parameter columns\n",
    "    param_cols = [col for col in trials_df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                  and not col.startswith('user_attr_')]\n",
    "    \n",
    "    # Identify best trials\n",
    "    best_df = trials_df.sort_values('value').head(highlight_n)\n",
    "    best_indices = best_df.index.tolist()\n",
    "    \n",
    "    # Create color column - 1 for best trials, 0 for others\n",
    "    trials_df['color'] = 0\n",
    "    trials_df.loc[best_indices, 'color'] = 1\n",
    "    \n",
    "    # Create figure\n",
    "    dimensions = []\n",
    "    \n",
    "    # Add parameter dimensions\n",
    "    for param in param_cols:\n",
    "        # Check if parameter is categorical\n",
    "        if trials_df[param].dtype == 'object' or trials_df[param].dtype == 'category':\n",
    "            # For categorical parameters, we can't use a numeric range\n",
    "            # Instead, create a dimension without specifying the range\n",
    "            dimensions.append(\n",
    "                dict(\n",
    "                    label=param,\n",
    "                    values=trials_df[param],\n",
    "                    tickvals=trials_df[param].unique().tolist()\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # For numerical parameters, use min and max for range\n",
    "            dimensions.append(\n",
    "                dict(\n",
    "                    range=[float(trials_df[param].min()), float(trials_df[param].max())],\n",
    "                    label=param,\n",
    "                    values=trials_df[param]\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Add objective value dimension\n",
    "    dimensions.append(\n",
    "        dict(\n",
    "            range=[float(trials_df['value'].min()), float(trials_df['value'].max())],\n",
    "            label='Objective Value',\n",
    "            values=trials_df['value']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create plot\n",
    "    fig = go.Figure(data=\n",
    "        go.Parcoords(\n",
    "            line=dict(\n",
    "                color=trials_df['color'],\n",
    "                colorscale=[[0, 'rgba(128, 128, 128, 0.2)'], [1, 'rgba(255, 0, 0, 1)']],\n",
    "                showscale=True,\n",
    "                colorbar=dict(\n",
    "                    title=f'Top {highlight_n} Trials',\n",
    "                    tickvals=[0, 1],\n",
    "                    ticktext=['Other Trials', f'Top {highlight_n} Trials']\n",
    "                )\n",
    "            ),\n",
    "            dimensions=dimensions\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Parallel Coordinates Plot (Highlighting Top {highlight_n} Trials)\",\n",
    "        height=600,\n",
    "        width=1200\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Hyperparameter Analysis\n",
    "\n",
    "# 1. Learning Curves Analysis\n",
    "def analyze_learning_curves():\n",
    "    \"\"\"Analyze the learning curves of the optimization process.\"\"\"\n",
    "    # Sort trials by execution order\n",
    "    sorted_trials = sorted(study.trials, key=lambda t: t.number)\n",
    "    \n",
    "    # Extract data\n",
    "    trial_numbers = []\n",
    "    best_values = []\n",
    "    values = []\n",
    "    \n",
    "    best_so_far = float('inf')  # Assuming minimization\n",
    "    for trial in sorted_trials:\n",
    "        if trial.state == optuna.trial.TrialState.COMPLETE:\n",
    "            trial_numbers.append(trial.number)\n",
    "            values.append(trial.value)\n",
    "            \n",
    "            # Update best value\n",
    "            if trial.value < best_so_far:\n",
    "                best_so_far = trial.value\n",
    "            \n",
    "            best_values.append(best_so_far)\n",
    "    \n",
    "    # Create plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add all trials\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trial_numbers,\n",
    "            y=values,\n",
    "            mode='markers',\n",
    "            name='Trial Values',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color='blue',\n",
    "                opacity=0.5\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add best value curve\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=trial_numbers,\n",
    "            y=best_values,\n",
    "            mode='lines',\n",
    "            name='Best Value',\n",
    "            line=dict(\n",
    "                color='red',\n",
    "                width=2\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add moving average\n",
    "    window_size = min(50, len(values) // 5) if len(values) >= 10 else 2\n",
    "    if window_size >= 2:\n",
    "        moving_avg = np.convolve(values, np.ones(window_size)/window_size, mode='valid')\n",
    "        ma_x = trial_numbers[window_size-1:]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ma_x,\n",
    "                y=moving_avg,\n",
    "                mode='lines',\n",
    "                name=f'Moving Average (window={window_size})',\n",
    "                line=dict(\n",
    "                    color='green',\n",
    "                    width=2,\n",
    "                    dash='dash'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Optimization Learning Curve\",\n",
    "        xaxis_title=\"Trial Number\",\n",
    "        yaxis_title=\"Objective Value\",\n",
    "        height=600,\n",
    "        width=1000\n",
    "    )\n",
    "    \n",
    "    # Add annotations for notable points\n",
    "    # 1. Initial best point\n",
    "    if len(trial_numbers) > 0:\n",
    "        fig.add_annotation(\n",
    "            x=trial_numbers[0],\n",
    "            y=best_values[0],\n",
    "            text=\"Initial Best\",\n",
    "            showarrow=True,\n",
    "            arrowhead=1\n",
    "        )\n",
    "    \n",
    "    # 2. Final best point\n",
    "    if len(trial_numbers) > 0:\n",
    "        final_idx = len(trial_numbers) - 1\n",
    "        fig.add_annotation(\n",
    "            x=trial_numbers[final_idx],\n",
    "            y=best_values[final_idx],\n",
    "            text=\"Final Best\",\n",
    "            showarrow=True,\n",
    "            arrowhead=1\n",
    "        )\n",
    "    \n",
    "    # 3. Biggest improvement\n",
    "    if len(best_values) > 1:\n",
    "        improvements = [best_values[i-1] - best_values[i] for i in range(1, len(best_values))]\n",
    "        if improvements:\n",
    "            max_imp_idx = np.argmax(improvements) + 1\n",
    "            fig.add_annotation(\n",
    "                x=trial_numbers[max_imp_idx],\n",
    "                y=best_values[max_imp_idx],\n",
    "                text=\"Largest Improvement\",\n",
    "                showarrow=True,\n",
    "                arrowhead=1\n",
    "            )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 2. Hyperparameter Convergence Analysis\n",
    "def analyze_hyperparameter_convergence(trials_df, param_names=None, window_size=20):\n",
    "    \"\"\"Analyze how hyperparameters converge during optimization.\"\"\"\n",
    "    if param_names is None:\n",
    "        # Try to get top parameters by importance\n",
    "        try:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            param_names = list(importance.keys())[:5]  # Top 5 parameters\n",
    "        except:\n",
    "            # Get all parameter names\n",
    "            param_names = [col for col in trials_df.columns if col not in \n",
    "                          ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                          and not col.startswith('user_attr_')]\n",
    "            param_names = param_names[:5]  # Limit to first 5\n",
    "    \n",
    "    # Sort by trial number\n",
    "    sorted_df = trials_df.sort_values('number')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = make_subplots(\n",
    "        rows=len(param_names), \n",
    "        cols=1,\n",
    "        subplot_titles=[f\"{param} Convergence\" for param in param_names],\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.05\n",
    "    )\n",
    "    \n",
    "    # Add traces for each parameter\n",
    "    for i, param in enumerate(param_names):\n",
    "        # Check if numerical parameter\n",
    "        if sorted_df[param].dtype in [np.float64, np.int64]:\n",
    "            # Calculate rolling statistics if enough trials\n",
    "            if len(sorted_df) >= window_size:\n",
    "                # Rolling mean\n",
    "                rolling_mean = sorted_df[param].rolling(window=window_size).mean()\n",
    "                \n",
    "                # Rolling standard deviation\n",
    "                rolling_std = sorted_df[param].rolling(window=window_size).std()\n",
    "                \n",
    "                # Add rolling mean\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=sorted_df['number'],\n",
    "                        y=rolling_mean,\n",
    "                        mode='lines',\n",
    "                        name=f'{param} Rolling Mean',\n",
    "                        line=dict(\n",
    "                            color='blue',\n",
    "                            width=2\n",
    "                        )\n",
    "                    ),\n",
    "                    row=i+1, col=1\n",
    "                )\n",
    "                \n",
    "                # Add confidence interval (mean  std)\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=np.concatenate([sorted_df['number'], sorted_df['number'][::-1]]),\n",
    "                        y=np.concatenate([rolling_mean + rolling_std, (rolling_mean - rolling_std)[::-1]]),\n",
    "                        fill='toself',\n",
    "                        fillcolor='rgba(0, 0, 255, 0.1)',\n",
    "                        line=dict(color='rgba(0, 0, 255, 0)'),\n",
    "                        name=f'{param}  '\n",
    "                    ),\n",
    "                    row=i+1, col=1\n",
    "                )\n",
    "            \n",
    "            # Add parameter values\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=sorted_df['number'],\n",
    "                    y=sorted_df[param],\n",
    "                    mode='markers',\n",
    "                    name=param,\n",
    "                    marker=dict(\n",
    "                        size=5,\n",
    "                        opacity=0.5,\n",
    "                        color=sorted_df['value'],\n",
    "                        colorscale='Viridis',\n",
    "                        colorbar=dict(title=\"Objective\"),\n",
    "                        showscale=i == 0  # Show colorbar only once\n",
    "                    )\n",
    "                ),\n",
    "                row=i+1, col=1\n",
    "            )\n",
    "            \n",
    "            # Add best trial marker\n",
    "            best_trial_idx = sorted_df['value'].idxmin()\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[sorted_df.loc[best_trial_idx, 'number']],\n",
    "                    y=[sorted_df.loc[best_trial_idx, param]],\n",
    "                    mode='markers',\n",
    "                    name=f'Best {param}',\n",
    "                    marker=dict(\n",
    "                        size=10,\n",
    "                        color='red',\n",
    "                        symbol='star'\n",
    "                    ),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=i+1, col=1\n",
    "            )\n",
    "        else:\n",
    "            # Categorical parameter - show frequency of each value over time\n",
    "            categories = sorted_df[param].unique()\n",
    "            \n",
    "            # Calculate frequency of each category in rolling window\n",
    "            if len(sorted_df) >= window_size:\n",
    "                for category in categories:\n",
    "                    # Calculate rolling frequency\n",
    "                    mask = (sorted_df[param] == category)\n",
    "                    rolling_freq = mask.rolling(window=window_size).mean()\n",
    "                    \n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=sorted_df['number'],\n",
    "                            y=rolling_freq,\n",
    "                            mode='lines',\n",
    "                            name=f'{param}={category}',\n",
    "                            line=dict(width=2)\n",
    "                        ),\n",
    "                        row=i+1, col=1\n",
    "                    )\n",
    "            \n",
    "            # Add markers for actual values\n",
    "            for category in categories:\n",
    "                cat_df = sorted_df[sorted_df[param] == category]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=cat_df['number'],\n",
    "                        y=np.ones(len(cat_df)) * 0.5,  # Middle of the plot\n",
    "                        mode='markers',\n",
    "                        name=f'{param}={category}',\n",
    "                        marker=dict(\n",
    "                            size=8,\n",
    "                            opacity=0.7\n",
    "                        )\n",
    "                    ),\n",
    "                    row=i+1, col=1\n",
    "                )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Hyperparameter Convergence Analysis (Window Size: {window_size})\",\n",
    "        height=300 * len(param_names),\n",
    "        width=1000,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update y-axis titles\n",
    "    for i, param in enumerate(param_names):\n",
    "        fig.update_yaxes(title_text=param, row=i+1, col=1)\n",
    "    \n",
    "    # Update x-axis title (only for the last subplot)\n",
    "    fig.update_xaxes(title_text=\"Trial Number\", row=len(param_names), col=1)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 3. Parallel Coordinates with Custom Highlighting\n",
    "def plot_enhanced_parallel_coordinate(trials_df, highlight_n=10):\n",
    "    \"\"\"Create an enhanced parallel coordinates plot with highlighting of best trials.\"\"\"\n",
    "    # Get parameter columns\n",
    "    param_cols = [col for col in trials_df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                  and not col.startswith('user_attr_')]\n",
    "    \n",
    "    # Identify best trials\n",
    "    best_df = trials_df.sort_values('value').head(highlight_n)\n",
    "    best_indices = best_df.index.tolist()\n",
    "    \n",
    "    # Create color column - 1 for best trials, 0 for others\n",
    "    trials_df['color'] = 0\n",
    "    trials_df.loc[best_indices, 'color'] = 1\n",
    "    \n",
    "    # Create figure\n",
    "    dimensions = []\n",
    "    \n",
    "    # Add parameter dimensions\n",
    "    for param in param_cols:\n",
    "        # Check if parameter is categorical\n",
    "        if trials_df[param].dtype == 'object' or trials_df[param].dtype == 'category':\n",
    "            # For categorical parameters, we can't use a numeric range\n",
    "            # Instead, create a dimension without specifying the range\n",
    "            dimensions.append(\n",
    "                dict(\n",
    "                    label=param,\n",
    "                    values=trials_df[param],\n",
    "                    tickvals=trials_df[param].unique().tolist()\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # For numerical parameters, use min and max for range\n",
    "            dimensions.append(\n",
    "                dict(\n",
    "                    range=[float(trials_df[param].min()), float(trials_df[param].max())],\n",
    "                    label=param,\n",
    "                    values=trials_df[param]\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Add objective value dimension\n",
    "    dimensions.append(\n",
    "        dict(\n",
    "            range=[float(trials_df['value'].min()), float(trials_df['value'].max())],\n",
    "            label='Objective Value',\n",
    "            values=trials_df['value']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create plot\n",
    "    fig = go.Figure(data=\n",
    "        go.Parcoords(\n",
    "            line=dict(\n",
    "                color=trials_df['color'],\n",
    "                colorscale=[[0, 'rgba(128, 128, 128, 0.2)'], [1, 'rgba(255, 0, 0, 1)']],\n",
    "                showscale=True,\n",
    "                colorbar=dict(\n",
    "                    title=f'Top {highlight_n} Trials',\n",
    "                    tickvals=[0, 1],\n",
    "                    ticktext=['Other Trials', f'Top {highlight_n} Trials']\n",
    "                )\n",
    "            ),\n",
    "            dimensions=dimensions\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Parallel Coordinates Plot (Highlighting Top {highlight_n} Trials)\",\n",
    "        height=600,\n",
    "        width=1200\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 4. Hyperparameter Optimization Recommendations\n",
    "def generate_optimization_recommendations(study, trials_df):\n",
    "    \"\"\"Generate recommendations for future hyperparameter optimization.\"\"\"\n",
    "    # Get parameter columns\n",
    "    param_cols = [col for col in trials_df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration', 'color'] \n",
    "                  and not col.startswith('user_attr_')]\n",
    "    \n",
    "    # Ensure we're only looking at actual hyperparameters (those found in best_trial.params)\n",
    "    best_trial = study.best_trial\n",
    "    actual_params = set(best_trial.params.keys())\n",
    "    param_cols = [param for param in param_cols if param in actual_params]\n",
    "    \n",
    "    # Get parameter distributions from best trials\n",
    "    top_n = min(20, len(trials_df))\n",
    "    top_df = trials_df.sort_values('value').head(top_n)\n",
    "    \n",
    "    # Get parameter importance\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        param_ranking = {param: rank for rank, (param, _) in enumerate(importance.items())}\n",
    "    except:\n",
    "        # If importance calculation fails, use simple statistics\n",
    "        param_ranking = {}\n",
    "        for param in param_cols:\n",
    "            if trials_df[param].dtype in [np.float64, np.int64]:\n",
    "                # Calculate correlation with objective\n",
    "                corr = abs(trials_df[[param, 'value']].corr().iloc[0, 1])\n",
    "                param_ranking[param] = corr\n",
    "            else:\n",
    "                # For categorical, use standard deviation of mean values per category\n",
    "                means = trials_df.groupby(param)['value'].mean()\n",
    "                param_ranking[param] = means.std()\n",
    "        \n",
    "        # Convert to ranks\n",
    "        sorted_params = sorted(param_ranking.items(), key=lambda x: x[1], reverse=True)\n",
    "        param_ranking = {param: rank for rank, (param, _) in enumerate(sorted_params)}\n",
    "    \n",
    "    # Get parameter statistics for numerical parameters\n",
    "    param_stats = {}\n",
    "    for param in param_cols:\n",
    "        if trials_df[param].dtype in [np.float64, np.int64]:\n",
    "            param_stats[param] = {\n",
    "                'min': trials_df[param].min(),\n",
    "                'max': trials_df[param].max(),\n",
    "                'mean': trials_df[param].mean(),\n",
    "                'std': trials_df[param].std(),\n",
    "                'best': best_trial.params[param],\n",
    "                'top_mean': top_df[param].mean(),\n",
    "                'top_std': top_df[param].std(),\n",
    "                'top_min': top_df[param].min(),\n",
    "                'top_max': top_df[param].max()\n",
    "            }\n",
    "        else:\n",
    "            # For categorical parameters\n",
    "            counts = trials_df[param].value_counts()\n",
    "            top_counts = top_df[param].value_counts()\n",
    "            \n",
    "            # Calculate frequency in top vs all\n",
    "            categories = {}\n",
    "            for cat in trials_df[param].unique():\n",
    "                all_freq = counts.get(cat, 0) / len(trials_df)\n",
    "                top_freq = top_counts.get(cat, 0) / len(top_df)\n",
    "                \n",
    "                # Ratio of frequencies (how much more common in top trials)\n",
    "                ratio = top_freq / all_freq if all_freq > 0 else 0\n",
    "                \n",
    "                categories[cat] = {\n",
    "                    'all_count': counts.get(cat, 0),\n",
    "                    'all_freq': all_freq,\n",
    "                    'top_count': top_counts.get(cat, 0),\n",
    "                    'top_freq': top_freq,\n",
    "                    'ratio': ratio\n",
    "                }\n",
    "            \n",
    "            param_stats[param] = {\n",
    "                'best': best_trial.params[param],\n",
    "                'categories': categories\n",
    "            }\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. Promising parameter ranges for numerical parameters\n",
    "    for param, stats in param_stats.items():\n",
    "        if isinstance(stats, dict) and 'top_min' in stats:\n",
    "            # For numerical parameters\n",
    "            if (stats['top_max'] - stats['top_min']) < 0.7 * (stats['max'] - stats['min']):\n",
    "                # If top trials use a narrower range\n",
    "                recommendations.append({\n",
    "                    'type': 'range_refinement',\n",
    "                    'param': param,\n",
    "                    'current_range': [stats['min'], stats['max']],\n",
    "                    'suggested_range': [stats['top_min'], stats['top_max']],\n",
    "                    'importance_rank': param_ranking.get(param, 999),\n",
    "                    'message': f\"Consider narrowing search range for {param} to [{stats['top_min']}, {stats['top_max']}]\"\n",
    "                })\n",
    "    \n",
    "    # 2. Promising categories for categorical parameters\n",
    "    for param, stats in param_stats.items():\n",
    "        if isinstance(stats, dict) and 'categories' in stats:\n",
    "            # For categorical parameters\n",
    "            best_categories = []\n",
    "            for cat, cat_stats in stats['categories'].items():\n",
    "                if cat_stats['ratio'] > 1.5 and cat_stats['top_count'] >= 2:\n",
    "                    best_categories.append((cat, cat_stats['ratio']))\n",
    "            \n",
    "            if best_categories:\n",
    "                best_cats = sorted(best_categories, key=lambda x: x[1], reverse=True)\n",
    "                cat_list = \", \".join([f\"{cat}\" for cat, _ in best_cats])\n",
    "                recommendations.append({\n",
    "                    'type': 'category_preference',\n",
    "                    'param': param,\n",
    "                    'preferred_categories': [cat for cat, _ in best_cats],\n",
    "                    'importance_rank': param_ranking.get(param, 999),\n",
    "                    'message': f\"Parameter {param} shows better results with values: {cat_list}\"\n",
    "                })\n",
    "    \n",
    "    # 3. Check if optimization is converging\n",
    "    # Get the last 25% of trials\n",
    "    n_trials = len(trials_df)\n",
    "    last_quarter = trials_df.sort_values('number').tail(n_trials // 4)\n",
    "    first_quarter = trials_df.sort_values('number').head(n_trials // 4)\n",
    "    \n",
    "    # Compare variance in parameters\n",
    "    for param in param_cols:\n",
    "        if trials_df[param].dtype in [np.float64, np.int64]:\n",
    "            first_std = first_quarter[param].std()\n",
    "            last_std = last_quarter[param].std()\n",
    "            \n",
    "            # If variance has decreased significantly\n",
    "            if last_std < 0.5 * first_std and param_ranking.get(param, 999) < len(param_cols) // 2:\n",
    "                recommendations.append({\n",
    "                    'type': 'convergence',\n",
    "                    'param': param,\n",
    "                    'first_std': first_std,\n",
    "                    'last_std': last_std,\n",
    "                    'importance_rank': param_ranking.get(param, 999),\n",
    "                    'message': f\"Parameter {param} is converging. Consider running more trials around current best value: {best_trial.params[param]}\"\n",
    "                })\n",
    "    \n",
    "    # 4. Performance plateau detection\n",
    "    best_values = []\n",
    "    best_so_far = float('inf')\n",
    "    for trial in sorted(trials_df.iterrows(), key=lambda x: x[1]['number']):\n",
    "        if trial[1]['value'] < best_so_far:\n",
    "            best_so_far = trial[1]['value']\n",
    "        best_values.append(best_so_far)\n",
    "    \n",
    "    # Check improvement in last quarter\n",
    "    if n_trials >= 20:\n",
    "        early_improvement = best_values[n_trials // 4] - best_values[0]\n",
    "        late_improvement = best_values[-1] - best_values[3 * n_trials // 4]\n",
    "        \n",
    "        if late_improvement < 0.1 * early_improvement:\n",
    "            recommendations.append({\n",
    "                'type': 'plateau',\n",
    "                'early_improvement': early_improvement,\n",
    "                'late_improvement': late_improvement,\n",
    "                'message': \"Optimization may be reaching a plateau. Consider changing the search space or trying different hyperparameters.\"\n",
    "            })\n",
    "    \n",
    "    # Sort recommendations by importance\n",
    "    recommendations.sort(key=lambda x: x.get('importance_rank', 999))\n",
    "    \n",
    "    return recommendations, param_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display visualizations and analysis\n",
    "print(\"## Optimization Learning Curve\")\n",
    "print(\"Analyzing how the optimization process learned over time...\")\n",
    "fig = analyze_learning_curves()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Hyperparameter Convergence Analysis\")\n",
    "print(\"Analyzing how hyperparameter values converged during optimization...\")\n",
    "fig = analyze_hyperparameter_convergence(trials_df, window_size=min(20, len(trials_df) // 5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Enhanced Parallel Coordinates Plot\")\n",
    "print(\"Visualizing parameter combinations with highlighted best trials...\")\n",
    "fig = plot_enhanced_parallel_coordinate(trials_df, highlight_n=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n## Hyperparameter Optimization Recommendations\")\n",
    "print(\"Generating recommendations for future optimization runs...\")\n",
    "recommendations, param_stats = generate_optimization_recommendations(study, trials_df)\n",
    "\n",
    "# Display recommendations\n",
    "for i, rec in enumerate(recommendations):\n",
    "    print(f\"\\n{i+1}. {rec['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed recommendations report\n",
    "if recommendations:\n",
    "    print(\"\\nDetailed Recommendations Report:\")\n",
    "    \n",
    "    # For range refinements\n",
    "    range_recs = [r for r in recommendations if r['type'] == 'range_refinement']\n",
    "    if range_recs:\n",
    "        print(\"\\nRecommended Parameter Range Refinements:\")\n",
    "        for rec in range_recs:\n",
    "            print(f\"  - {rec['param']}: Current [{rec['current_range'][0]}, {rec['current_range'][1]}]  \"\n",
    "                 f\"Suggested [{rec['suggested_range'][0]}, {rec['suggested_range'][1]}]\")\n",
    "    \n",
    "    # For category preferences\n",
    "    cat_recs = [r for r in recommendations if r['type'] == 'category_preference']\n",
    "    if cat_recs:\n",
    "        print(\"\\nRecommended Categorical Parameter Preferences:\")\n",
    "        for rec in cat_recs:\n",
    "            cats = \", \".join(rec['preferred_categories'])\n",
    "            print(f\"  - {rec['param']}: Prefer values {cats}\")\n",
    "    \n",
    "    # For convergence insights\n",
    "    conv_recs = [r for r in recommendations if r['type'] == 'convergence']\n",
    "    if conv_recs:\n",
    "        print(\"\\nConverging Parameters:\")\n",
    "        for rec in conv_recs:\n",
    "            print(f\"  - {rec['param']}: Focus near best value {study.best_params[rec['param']]}\")\n",
    "    \n",
    "    # Overall strategy recommendations\n",
    "    plateau_recs = [r for r in recommendations if r['type'] == 'plateau']\n",
    "    if plateau_recs:\n",
    "        print(\"\\nStrategy Recommendations:\")\n",
    "        for rec in plateau_recs:\n",
    "            print(f\"  - {rec['message']}\")\n",
    "else:\n",
    "    print(\"\\nNo specific recommendations generated. The current optimization strategy appears effective.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot recommendations visualizations for numerical parameters\n",
    "range_recs = [r for r in recommendations if r['type'] == 'range_refinement']\n",
    "if range_recs:\n",
    "    # Limit the number of plots to avoid overcrowding \n",
    "    max_plots = 10\n",
    "    if len(range_recs) > max_plots:\n",
    "        print(f\"Showing visualizations for the top {max_plots} parameter range recommendations (out of {len(range_recs)} total)\")\n",
    "        # Sort by importance and take top ones\n",
    "        range_recs = sorted(range_recs, key=lambda r: r.get('importance_rank', 999))[:max_plots]\n",
    "    \n",
    "    # Calculate appropriate vertical spacing based on number of recommendations\n",
    "    vertical_spacing = min(0.1, 0.9 / len(range_recs)) if len(range_recs) > 1 else 0.1\n",
    "    \n",
    "    # Create visualization for recommended ranges\n",
    "    fig = make_subplots(\n",
    "        rows=len(range_recs), \n",
    "        cols=1,\n",
    "        subplot_titles=[f\"{r['param']} Range Refinement\" for r in range_recs],\n",
    "        vertical_spacing=vertical_spacing\n",
    "    )\n",
    "    \n",
    "    for i, rec in enumerate(range_recs):\n",
    "        param = rec['param']\n",
    "        \n",
    "        # Plot all trials\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=trials_df[param],\n",
    "                nbinsx=30,\n",
    "                marker_color='lightblue',\n",
    "                opacity=0.7,\n",
    "                name=\"All Trials\"\n",
    "            ),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "        \n",
    "        # Plot best trials\n",
    "        top_df = trials_df.sort_values('value').head(20)\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=top_df[param],\n",
    "                nbinsx=15,\n",
    "                marker_color='red',\n",
    "                opacity=0.7,\n",
    "                name=\"Top Trials\"\n",
    "            ),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add vertical line for best value\n",
    "        fig.add_vline(\n",
    "            x=study.best_params[param],\n",
    "            line_dash=\"dash\",\n",
    "            line_color=\"green\",\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add suggested range\n",
    "        fig.add_vrect(\n",
    "            x0=rec['suggested_range'][0],\n",
    "            x1=rec['suggested_range'][1],\n",
    "            fillcolor=\"rgba(0, 255, 0, 0.2)\",\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add current range\n",
    "        fig.add_vrect(\n",
    "            x0=rec['current_range'][0],\n",
    "            x1=rec['current_range'][1],\n",
    "            fillcolor=\"rgba(0, 0, 255, 0.1)\",\n",
    "            layer=\"below\",\n",
    "            line_width=0,\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Recommended Parameter Range Refinements\",\n",
    "        height=300 * min(len(range_recs), 10),  # Limit height for many plots\n",
    "        width=1000,\n",
    "        showlegend=i == 0  # Show legend only once\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot categorical parameter recommendations\n",
    "cat_recs = [r for r in recommendations if r['type'] == 'category_preference']\n",
    "if cat_recs:\n",
    "    # Create visualization for categorical preferences\n",
    "    fig = make_subplots(\n",
    "        rows=len(cat_recs), \n",
    "        cols=1,\n",
    "        subplot_titles=[f\"{r['param']} Category Preferences\" for r in cat_recs],\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    for i, rec in enumerate(cat_recs):\n",
    "        param = rec['param']\n",
    "        param_stats_dict = param_stats[param]\n",
    "        \n",
    "        # Prepare data\n",
    "        categories = []\n",
    "        all_freqs = []\n",
    "        top_freqs = []\n",
    "        \n",
    "        for cat, stats in param_stats_dict['categories'].items():\n",
    "            categories.append(str(cat))\n",
    "            all_freqs.append(stats['all_freq'])\n",
    "            top_freqs.append(stats['top_freq'])\n",
    "        \n",
    "        # Sort by top frequency\n",
    "        sorted_indices = np.argsort(top_freqs)[::-1]\n",
    "        categories = [categories[j] for j in sorted_indices]\n",
    "        all_freqs = [all_freqs[j] for j in sorted_indices]\n",
    "        top_freqs = [top_freqs[j] for j in sorted_indices]\n",
    "        \n",
    "        # Add bar for all trials\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=categories,\n",
    "                y=all_freqs,\n",
    "                name=\"All Trials\",\n",
    "                marker_color='lightblue'\n",
    "            ),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add bar for top trials\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=categories,\n",
    "                y=top_freqs,\n",
    "                name=\"Top Trials\",\n",
    "                marker_color='red'\n",
    "            ),\n",
    "            row=i+1, col=1\n",
    "        )\n",
    "        \n",
    "        # Highlight best value\n",
    "        best_value = study.best_params[param]\n",
    "        if str(best_value) in categories:\n",
    "            idx = categories.index(str(best_value))\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[categories[idx]],\n",
    "                    y=[max(all_freqs[idx], top_freqs[idx]) + 0.05],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=12,\n",
    "                        color='green',\n",
    "                        symbol='star'\n",
    "                    ),\n",
    "                    name=\"Best Value\"\n",
    "                ),\n",
    "                row=i+1, col=1\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=\"Categorical Parameter Preferences\",\n",
    "        height=300 * len(cat_recs),\n",
    "        width=1000,\n",
    "        showlegend=i == 0  # Show legend only once\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictive Modeling and Next Steps\n",
    "\n",
    "# 1. Surrogate modeling of the objective function\n",
    "def build_surrogate_model(trials_df):\n",
    "    \"\"\"Build a surrogate model to predict objective values based on hyperparameters.\"\"\"\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.model_selection import cross_val_score, train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    \n",
    "    # Get parameter columns\n",
    "    param_cols = [col for col in trials_df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                  and not col.startswith('user_attr_')]\n",
    "    \n",
    "    # Split features and target\n",
    "    X = trials_df[param_cols]\n",
    "    y = trials_df['value']\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_cols),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Create and evaluate RandomForest model\n",
    "    rf_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Create and evaluate GradientBoosting model\n",
    "    gb_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Split data for evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    models = {\n",
    "        'Random Forest': rf_model,\n",
    "        'Gradient Boosting': gb_model\n",
    "    }\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_preds = model.predict(X_train)\n",
    "        test_preds = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_mse = mean_squared_error(y_train, train_preds)\n",
    "        test_mse = mean_squared_error(y_test, test_preds)\n",
    "        train_r2 = r2_score(y_train, train_preds)\n",
    "        test_r2 = r2_score(y_test, test_preds)\n",
    "        \n",
    "        model_results[name] = {\n",
    "            'model': model,\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2\n",
    "        }\n",
    "    \n",
    "    # Select best model (based on test R)\n",
    "    best_model_name = max(model_results.items(), key=lambda x: x[1]['test_r2'])[0]\n",
    "    best_model = model_results[best_model_name]\n",
    "    \n",
    "    print(f\"Best surrogate model: {best_model_name}\")\n",
    "    print(f\"  Training MSE: {best_model['train_mse']:.6f}\")\n",
    "    print(f\"  Test MSE: {best_model['test_mse']:.6f}\")\n",
    "    print(f\"  Training R: {best_model['train_r2']:.6f}\")\n",
    "    print(f\"  Test R: {best_model['test_r2']:.6f}\")\n",
    "    \n",
    "    return best_model['model'], model_results, param_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature importance from surrogate model\n",
    "def plot_surrogate_model_importance(model, param_cols, model_results):\n",
    "    \"\"\"Extract and visualize feature importance from the surrogate model.\"\"\"\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get the actual model from the pipeline\n",
    "    if hasattr(model, 'named_steps') and 'model' in model.named_steps:\n",
    "        model_name = type(model.named_steps['model']).__name__\n",
    "        \n",
    "        # Try to extract feature importances directly if available\n",
    "        if hasattr(model.named_steps['model'], 'feature_importances_'):\n",
    "            importances = model.named_steps['model'].feature_importances_\n",
    "            \n",
    "            # Get feature names after preprocessing (tricky with one-hot encoding)\n",
    "            # For simplicity, we'll use original parameter names\n",
    "            feature_names = param_cols\n",
    "            \n",
    "            # Create sorted indices\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "            \n",
    "            # Limit to top 15 features\n",
    "            n_features = min(15, len(param_cols))\n",
    "            \n",
    "            # Create figure\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=importances[indices[:n_features]],\n",
    "                    y=[param_cols[i] for i in indices[:n_features]],\n",
    "                    orientation='h',\n",
    "                    marker_color='navy'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"Feature Importance from {model_name}\",\n",
    "                xaxis_title=\"Importance\",\n",
    "                yaxis_title=\"Parameter\",\n",
    "                height=500,\n",
    "                width=800\n",
    "            )\n",
    "            \n",
    "            return fig\n",
    "    \n",
    "    # If direct feature importance is not available or extraction is complex,\n",
    "    # return a message instead\n",
    "    print(\"Feature importance extraction not available for this model type.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predict promising regions for additional exploration\n",
    "def predict_promising_regions(model, param_cols, trials_df, n_samples=10000, n_top=20):\n",
    "    \"\"\"Use the surrogate model to predict promising regions for additional trials.\"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    \n",
    "    # Get min/max ranges for numerical parameters and values for categorical\n",
    "    param_ranges = {}\n",
    "    for param in param_cols:\n",
    "        if trials_df[param].dtype in [np.float64, np.int64]:\n",
    "            param_ranges[param] = {\n",
    "                'type': 'numerical',\n",
    "                'min': trials_df[param].min(),\n",
    "                'max': trials_df[param].max()\n",
    "            }\n",
    "        else:\n",
    "            param_ranges[param] = {\n",
    "                'type': 'categorical',\n",
    "                'values': trials_df[param].unique().tolist()\n",
    "            }\n",
    "    \n",
    "    # Create random samples within parameter ranges\n",
    "    samples = {}\n",
    "    for param, range_info in param_ranges.items():\n",
    "        if range_info['type'] == 'numerical':\n",
    "            samples[param] = np.random.uniform(\n",
    "                range_info['min'],\n",
    "                range_info['max'],\n",
    "                n_samples\n",
    "            )\n",
    "            \n",
    "            # Convert to int if the original was int\n",
    "            if trials_df[param].dtype == np.int64:\n",
    "                samples[param] = np.round(samples[param]).astype(np.int64)\n",
    "        else:\n",
    "            samples[param] = np.random.choice(\n",
    "                range_info['values'],\n",
    "                n_samples\n",
    "            )\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    samples_df = pd.DataFrame(samples)\n",
    "    \n",
    "    # Predict objective values\n",
    "    predictions = model.predict(samples_df)\n",
    "    \n",
    "    # Add predictions to DataFrame\n",
    "    samples_df['predicted_value'] = predictions\n",
    "    \n",
    "    # Sort by predicted value (assuming minimization)\n",
    "    sorted_samples = samples_df.sort_values('predicted_value')\n",
    "    \n",
    "    # Get top predicted configurations\n",
    "    top_configs = sorted_samples.head(n_top)\n",
    "    \n",
    "    return top_configs, sorted_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualization of predicted promising regions\n",
    "def visualize_promising_regions(top_configs, all_samples, trials_df, param_cols, top_n=5):\n",
    "    \"\"\"Visualize the most promising regions identified by the surrogate model.\"\"\"\n",
    "    # Select top parameters by importance\n",
    "    if len(param_cols) > 8:\n",
    "        try:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            param_cols = list(importance.keys())[:8]  # Top 8 parameters\n",
    "        except:\n",
    "            param_cols = param_cols[:8]  # First 8 parameters\n",
    "    \n",
    "    # 1. Parallel Coordinates Plot for top configurations\n",
    "    fig_parallel = go.Figure(data=\n",
    "        go.Parcoords(\n",
    "            line=dict(\n",
    "                color=top_configs['predicted_value'],\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Predicted Value\")\n",
    "            ),\n",
    "            dimensions=[\n",
    "                dict(\n",
    "                    range=[top_configs[param].min(), top_configs[param].max()],\n",
    "                    label=param,\n",
    "                    values=top_configs[param]\n",
    "                ) for param in param_cols\n",
    "            ] + [\n",
    "                dict(\n",
    "                    range=[top_configs['predicted_value'].min(), top_configs['predicted_value'].max()],\n",
    "                    label='Predicted Value',\n",
    "                    values=top_configs['predicted_value']\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig_parallel.update_layout(\n",
    "        title=\"Parallel Coordinates Plot of Top Predicted Configurations\",\n",
    "        height=600,\n",
    "        width=1200\n",
    "    )\n",
    "    \n",
    "    # 2. Scatter plots for most important parameter pairs\n",
    "    # Select top 2 numeric parameters\n",
    "    numeric_params = [p for p in param_cols if trials_df[p].dtype in [np.float64, np.int64]]\n",
    "    if len(numeric_params) >= 2:\n",
    "        top_numeric = numeric_params[:2]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        fig_scatter = go.Figure()\n",
    "        \n",
    "        # Plot all samples\n",
    "        if len(all_samples) > 2000:\n",
    "            # Subsample for better visualization\n",
    "            plot_samples = all_samples.sample(2000, random_state=42)\n",
    "        else:\n",
    "            plot_samples = all_samples\n",
    "        \n",
    "        fig_scatter.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_samples[top_numeric[0]],\n",
    "                y=plot_samples[top_numeric[1]],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=5,\n",
    "                    color=plot_samples['predicted_value'],\n",
    "                    colorscale='Viridis',\n",
    "                    colorbar=dict(title=\"Predicted Value\"),\n",
    "                    opacity=0.5\n",
    "                ),\n",
    "                name='Predicted Samples'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Plot actual trials colored by real value\n",
    "        fig_scatter.add_trace(\n",
    "            go.Scatter(\n",
    "                x=trials_df[top_numeric[0]],\n",
    "                y=trials_df[top_numeric[1]],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=trials_df['value'],\n",
    "                    colorscale='Viridis',\n",
    "                    opacity=0.8,\n",
    "                    line=dict(color='black', width=1)\n",
    "                ),\n",
    "                name='Actual Trials'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Highlight top predicted configurations\n",
    "        fig_scatter.add_trace(\n",
    "            go.Scatter(\n",
    "                x=top_configs[top_numeric[0]].head(top_n),\n",
    "                y=top_configs[top_numeric[1]].head(top_n),\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=12,\n",
    "                    color='red',\n",
    "                    symbol='star',\n",
    "                    line=dict(color='black', width=1)\n",
    "                ),\n",
    "                name=f'Top {top_n} Predicted'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Highlight best actual trial\n",
    "        best_trial_idx = trials_df['value'].idxmin()\n",
    "        fig_scatter.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[trials_df.loc[best_trial_idx, top_numeric[0]]],\n",
    "                y=[trials_df.loc[best_trial_idx, top_numeric[1]]],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=15,\n",
    "                    color='green',\n",
    "                    symbol='circle',\n",
    "                    line=dict(color='black', width=2)\n",
    "                ),\n",
    "                name='Best Actual Trial'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig_scatter.update_layout(\n",
    "            title=f\"Predicted Promising Regions in {top_numeric[0]} vs {top_numeric[1]} Space\",\n",
    "            xaxis_title=top_numeric[0],\n",
    "            yaxis_title=top_numeric[1],\n",
    "            height=800,\n",
    "            width=1000\n",
    "        )\n",
    "        \n",
    "        return fig_parallel, fig_scatter\n",
    "    \n",
    "    return fig_parallel, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Generate next steps recommendations\n",
    "def generate_next_steps_recommendations(trials_df, top_configs):\n",
    "    \"\"\"Generate recommendations for next steps in the hyperparameter optimization process.\"\"\"\n",
    "    # Calculate the number of trials\n",
    "    n_trials = len(trials_df)\n",
    "    \n",
    "    # Calculate improvement over time\n",
    "    trials_sorted = trials_df.sort_values('number')\n",
    "    best_values = []\n",
    "    best_so_far = float('inf')\n",
    "    \n",
    "    for _, trial in trials_sorted.iterrows():\n",
    "        if trial['value'] < best_so_far:\n",
    "            best_so_far = trial['value']\n",
    "        best_values.append(best_so_far)\n",
    "    \n",
    "    # Calculate relative improvement in last quarter\n",
    "    if n_trials >= 20:\n",
    "        early_improvement = (best_values[n_trials // 4] - best_values[0]) / abs(best_values[0]) if best_values[0] != 0 else 1\n",
    "        late_improvement = (best_values[-1] - best_values[3 * n_trials // 4]) / abs(best_values[3 * n_trials // 4]) if best_values[3 * n_trials // 4] != 0 else 0\n",
    "        \n",
    "        improvement_ratio = late_improvement / early_improvement if early_improvement != 0 else 0\n",
    "    else:\n",
    "        improvement_ratio = 1  # Not enough trials to calculate\n",
    "    \n",
    "    # Generate recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    # 1. Overall study status\n",
    "    if improvement_ratio < 0.1:\n",
    "        # Little recent improvement\n",
    "        recommendations.append({\n",
    "            'category': 'study_status',\n",
    "            'title': 'Optimization Plateau Detected',\n",
    "            'description': 'Recent trials show minimal improvement compared to early trials. Consider exploring new hyperparameters or changing the search strategy.',\n",
    "            'actions': [\n",
    "                'Try a different optimization algorithm (e.g., switch between TPE, CMA-ES, or Random)',\n",
    "                'Expand the search space with additional hyperparameters',\n",
    "                'Consider a different model architecture or approach'\n",
    "            ]\n",
    "        })\n",
    "    elif improvement_ratio < 0.3:\n",
    "        # Moderate recent improvement\n",
    "        recommendations.append({\n",
    "            'category': 'study_status',\n",
    "            'title': 'Optimization Slowing Down',\n",
    "            'description': 'Improvement rate is declining but still showing progress. A more focused search might help.',\n",
    "            'actions': [\n",
    "                'Narrow search space around promising regions',\n",
    "                'Increase the number of trials for more exploration',\n",
    "                'Consider more advanced pruning strategies to eliminate poor trials early'\n",
    "            ]\n",
    "        })\n",
    "    else:\n",
    "        # Good recent improvement\n",
    "        recommendations.append({\n",
    "            'category': 'study_status',\n",
    "            'title': 'Optimization Progressing Well',\n",
    "            'description': 'The optimization is showing good progress with recent improvements.',\n",
    "            'actions': [\n",
    "                'Continue with current strategy',\n",
    "                'Consider running more trials to further improve results',\n",
    "                'Fine-tune around the best configurations found so far'\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    # 2. Sample size recommendations\n",
    "    if n_trials < 100:\n",
    "        recommendations.append({\n",
    "            'category': 'sample_size',\n",
    "            'title': 'Consider Running More Trials',\n",
    "            'description': f'Current study has {n_trials} trials. For complex hyperparameter spaces, running more trials often leads to better results.',\n",
    "            'actions': [\n",
    "                f'Aim for at least 100-200 trials for reliable results',\n",
    "                'Use distributed optimization to run trials in parallel if possible',\n",
    "                'Consider early stopping of unpromising trials to save compute resources'\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    # 3. Exploration vs exploitation balance\n",
    "    param_cols = [col for col in trials_df.columns if col not in ['number', 'value', 'datetime_start', 'datetime_complete', 'duration'] \n",
    "                  and not col.startswith('user_attr_')]\n",
    "    \n",
    "    # Check parameter space coverage\n",
    "    coverage_metrics = []\n",
    "    for param in param_cols:\n",
    "        if trials_df[param].dtype in [np.float64, np.int64]:\n",
    "            # For numerical parameters, check quartile coverage\n",
    "            q1 = trials_df[param].quantile(0.25)\n",
    "            q3 = trials_df[param].quantile(0.75)\n",
    "            param_range = trials_df[param].max() - trials_df[param].min()\n",
    "            \n",
    "            if param_range > 0:\n",
    "                quartile_width = (q3 - q1) / param_range\n",
    "                coverage_metrics.append(quartile_width)\n",
    "    \n",
    "    # Calculate average coverage metric\n",
    "    avg_coverage = np.mean(coverage_metrics) if coverage_metrics else 0.5\n",
    "    \n",
    "    if avg_coverage < 0.3:\n",
    "        # Wide exploration\n",
    "        recommendations.append({\n",
    "            'category': 'exploration',\n",
    "            'title': 'Wide Parameter Space Exploration',\n",
    "            'description': 'The current study shows very wide parameter exploration. Consider focusing on promising regions.',\n",
    "            'actions': [\n",
    "                'Use surrogate modeling to identify promising regions',\n",
    "                'Run a focused study around the best parameters found so far',\n",
    "                'Adjust sampler settings to favor exploitation over exploration'\n",
    "            ]\n",
    "        })\n",
    "    elif avg_coverage > 0.7:\n",
    "        # Narrow exploration\n",
    "        recommendations.append({\n",
    "            'category': 'exploration',\n",
    "            'title': 'Narrow Parameter Space Exploration',\n",
    "            'description': 'The current study shows very concentrated parameter exploration. Consider widening the search.',\n",
    "            'actions': [\n",
    "                'Expand parameter ranges to explore more diverse configurations',\n",
    "                'Try random sampling for some trials to break out of local optima',\n",
    "                'Add randomness to the search process'\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    # 4. Specific next trial recommendations\n",
    "    recommendations.append({\n",
    "        'category': 'next_trials',\n",
    "        'title': 'Suggested Next Trial Configurations',\n",
    "        'description': 'Based on surrogate modeling, the following configurations are predicted to perform well:',\n",
    "        'configs': top_configs.head(5).to_dict('records')\n",
    "    })\n",
    "    \n",
    "    # 5. Advanced techniques to consider\n",
    "    recommendations.append({\n",
    "        'category': 'techniques',\n",
    "        'title': 'Advanced Optimization Techniques',\n",
    "        'description': 'Consider these advanced techniques to further improve results:',\n",
    "        'actions': [\n",
    "            'Multi-objective optimization if you have multiple metrics to balance',\n",
    "            'Population-based training for evolutionary optimization',\n",
    "            'Bayesian optimization with custom kernels',\n",
    "            'Meta-learning from previous similar studies',\n",
    "            'Hyperband or successive halving for efficient resource allocation'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute analysis and recommendations\n",
    "print(\"## Building Surrogate Model of the Objective Function\")\n",
    "print(\"Training machine learning models to predict performance based on hyperparameters...\")\n",
    "\n",
    "# Ensure we have enough trials for modeling\n",
    "if len(trials_df) >= 20:\n",
    "    surrogate_model, model_results, param_cols = build_surrogate_model(trials_df)\n",
    "    \n",
    "    print(\"\\n## Surrogate Model Feature Importance\")\n",
    "    print(\"Analyzing which parameters most strongly influence the predicted performance...\")\n",
    "    importance_fig = plot_surrogate_model_importance(surrogate_model, param_cols, model_results)\n",
    "    if importance_fig:\n",
    "        importance_fig.show()\n",
    "    \n",
    "    print(\"\\n## Predicting Promising Hyperparameter Regions\")\n",
    "    print(\"Using the surrogate model to predict promising configurations for further exploration...\")\n",
    "    \n",
    "    # Predict promising regions\n",
    "    n_samples = min(20000, 10 * len(param_cols) * len(param_cols) * 100)\n",
    "    top_configs, all_samples = predict_promising_regions(surrogate_model, param_cols, trials_df, n_samples=n_samples)\n",
    "    \n",
    "    # Display top predicted configurations\n",
    "    print(\"\\nTop 5 predicted configurations:\")\n",
    "    display(top_configs.head(5)[param_cols + ['predicted_value']])\n",
    "    \n",
    "    # Visualize promising regions\n",
    "    fig_parallel, fig_scatter = visualize_promising_regions(top_configs, all_samples, trials_df, param_cols)\n",
    "    fig_parallel.show()\n",
    "    if fig_scatter:\n",
    "        fig_scatter.show()\n",
    "    \n",
    "    print(\"\\n## Next Steps Recommendations\")\n",
    "    print(\"Generating recommendations for continuing the hyperparameter optimization process...\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    next_steps = generate_next_steps_recommendations(trials_df, top_configs)\n",
    "    \n",
    "    # Display recommendations\n",
    "    for rec in next_steps:\n",
    "        print(f\"\\n### {rec['title']}\")\n",
    "        print(rec['description'])\n",
    "        \n",
    "        if 'actions' in rec:\n",
    "            print(\"\\nRecommended actions:\")\n",
    "            for i, action in enumerate(rec['actions']):\n",
    "                print(f\"  {i+1}. {action}\")\n",
    "        \n",
    "        if 'configs' in rec:\n",
    "            print(\"\\nTop configurations to try:\")\n",
    "            for i, config in enumerate(rec['configs']):\n",
    "                print(f\"\\nConfiguration {i+1} (Predicted value: {config['predicted_value']:.6f}):\")\n",
    "                for param, value in config.items():\n",
    "                    if param != 'predicted_value':\n",
    "                        print(f\"  {param}: {value}\")\n",
    "else:\n",
    "    print(\"Not enough completed trials for surrogate modeling. At least 20 trials are recommended.\")\n",
    "    print(\"Consider running more trials and then revisiting this analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Export\n",
    "\n",
    "# 1. Generate overall study summary\n",
    "def generate_study_summary(study, trials_df):\n",
    "    \"\"\"Generate a comprehensive summary of the study results.\"\"\"\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    \n",
    "    # Basic study information\n",
    "    summary = {\n",
    "        \"study_name\": study.study_name,\n",
    "        \"direction\": \"MINIMIZE\" if study.direction == optuna.study.StudyDirection.MINIMIZE else \"MAXIMIZE\",\n",
    "        \"total_trials\": len(study.trials),\n",
    "        \"completed_trials\": len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE])),\n",
    "        \"pruned_trials\": len(study.get_trials(states=[optuna.trial.TrialState.PRUNED])),\n",
    "        \"failed_trials\": len(study.get_trials(states=[optuna.trial.TrialState.FAIL])),\n",
    "        \"datetime_start\": min(t.datetime_start for t in study.trials if t.datetime_start is not None).isoformat() if study.trials else None,\n",
    "        \"datetime_end\": max(t.datetime_complete for t in study.trials if t.datetime_complete is not None).isoformat() if study.trials else None,\n",
    "        \"total_duration_hours\": None,\n",
    "        \"best_value\": study.best_value,\n",
    "        \"best_params\": study.best_params,\n",
    "        \"best_trial_number\": study.best_trial.number,\n",
    "    }\n",
    "    \n",
    "    # Calculate total duration\n",
    "    if summary[\"datetime_start\"] and summary[\"datetime_end\"]:\n",
    "        start_time = datetime.fromisoformat(summary[\"datetime_start\"])\n",
    "        end_time = datetime.fromisoformat(summary[\"datetime_end\"])\n",
    "        duration_seconds = (end_time - start_time).total_seconds()\n",
    "        summary[\"total_duration_hours\"] = duration_seconds / 3600\n",
    "    \n",
    "    # Parameter statistics\n",
    "    param_names = list(study.best_params.keys())\n",
    "    param_stats = {}\n",
    "    \n",
    "    for param in param_names:\n",
    "        if param in trials_df.columns:\n",
    "            if trials_df[param].dtype in [np.float64, np.int64]:\n",
    "                # Numerical parameter\n",
    "                param_stats[param] = {\n",
    "                    \"type\": \"numerical\",\n",
    "                    \"min\": trials_df[param].min(),\n",
    "                    \"max\": trials_df[param].max(),\n",
    "                    \"mean\": trials_df[param].mean(),\n",
    "                    \"median\": trials_df[param].median(),\n",
    "                    \"std\": trials_df[param].std(),\n",
    "                    \"best_value\": study.best_params[param]\n",
    "                }\n",
    "            else:\n",
    "                # Categorical parameter\n",
    "                value_counts = trials_df[param].value_counts().to_dict()\n",
    "                param_stats[param] = {\n",
    "                    \"type\": \"categorical\",\n",
    "                    \"value_counts\": value_counts,\n",
    "                    \"most_common\": trials_df[param].value_counts().index[0],\n",
    "                    \"best_value\": study.best_params[param]\n",
    "                }\n",
    "    \n",
    "    summary[\"parameter_stats\"] = param_stats\n",
    "    \n",
    "    # Performance statistics\n",
    "    summary[\"performance_stats\"] = {\n",
    "        \"min_value\": trials_df[\"value\"].min(),\n",
    "        \"max_value\": trials_df[\"value\"].max(),\n",
    "        \"mean_value\": trials_df[\"value\"].mean(),\n",
    "        \"median_value\": trials_df[\"value\"].median(),\n",
    "        \"std_value\": trials_df[\"value\"].std(),\n",
    "        \"q1_value\": trials_df[\"value\"].quantile(0.25),\n",
    "        \"q3_value\": trials_df[\"value\"].quantile(0.75)\n",
    "    }\n",
    "    \n",
    "    # Parameter importance if available\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        summary[\"parameter_importance\"] = {param: float(score) for param, score in importance.items()}\n",
    "    except Exception as e:\n",
    "        summary[\"parameter_importance\"] = {\"error\": str(e)}\n",
    "    \n",
    "    # Top 10 trials\n",
    "    top_trials = trials_df.sort_values(\"value\").head(10)\n",
    "    summary[\"top_trials\"] = []\n",
    "    \n",
    "    for _, trial in top_trials.iterrows():\n",
    "        trial_params = {param: trial[param] for param in param_names if param in trial}\n",
    "        summary[\"top_trials\"].append({\n",
    "            \"trial_number\": int(trial[\"number\"]),\n",
    "            \"value\": float(trial[\"value\"]),\n",
    "            \"params\": trial_params\n",
    "        })\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create HTML report\n",
    "def create_html_report(study, trials_df, summary):\n",
    "    \"\"\"Create a detailed HTML report of the hyperparameter optimization results.\"\"\"\n",
    "    from datetime import datetime\n",
    "    import base64\n",
    "    import io\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.figure import Figure\n",
    "    \n",
    "    # Function to encode matplotlib figure to base64 for HTML embedding\n",
    "    def fig_to_base64(fig):\n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf, format='png', dpi=100, bbox_inches='tight')\n",
    "        buf.seek(0)\n",
    "        img_str = base64.b64encode(buf.read()).decode('utf-8')\n",
    "        return img_str\n",
    "    \n",
    "    # Generate key visualizations\n",
    "    # 1. Optimization history\n",
    "    try:\n",
    "        fig = Figure(figsize=(10, 6))\n",
    "        ax = fig.subplots()\n",
    "        \n",
    "        # Extract data\n",
    "        trial_numbers = [t.number for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        values = [t.value for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        \n",
    "        # Plot\n",
    "        ax.scatter(trial_numbers, values, alpha=0.5)\n",
    "        ax.set_xlabel('Trial Number')\n",
    "        ax.set_ylabel('Objective Value')\n",
    "        ax.set_title('Optimization History')\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        optimization_history_img = fig_to_base64(fig)\n",
    "    except Exception as e:\n",
    "        optimization_history_img = None\n",
    "        print(f\"Could not generate optimization history plot: {e}\")\n",
    "    \n",
    "    # 2. Parameter importances\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        params = list(importance.keys())\n",
    "        scores = list(importance.values())\n",
    "        \n",
    "        fig = Figure(figsize=(10, 6))\n",
    "        ax = fig.subplots()\n",
    "        \n",
    "        y_pos = np.arange(len(params))\n",
    "        ax.barh(y_pos, scores, align='center')\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(params)\n",
    "        ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "        ax.set_xlabel('Importance Score')\n",
    "        ax.set_title('Parameter Importance')\n",
    "        \n",
    "        param_importance_img = fig_to_base64(fig)\n",
    "    except Exception as e:\n",
    "        param_importance_img = None\n",
    "        print(f\"Could not generate parameter importance plot: {e}\")\n",
    "    \n",
    "    # 3. Top parameters distribution\n",
    "    try:\n",
    "        if summary[\"parameter_importance\"] and not isinstance(summary[\"parameter_importance\"], dict):\n",
    "            # Get top 5 parameters\n",
    "            top_params = list(summary[\"parameter_importance\"].keys())[:5]\n",
    "            \n",
    "            fig = Figure(figsize=(15, 10))\n",
    "            axs = fig.subplots(len(top_params), 1)\n",
    "            \n",
    "            for i, param in enumerate(top_params):\n",
    "                ax = axs[i] if len(top_params) > 1 else axs\n",
    "                \n",
    "                if summary[\"parameter_stats\"][param][\"type\"] == \"numerical\":\n",
    "                    # Histogram for numerical parameters\n",
    "                    ax.hist(trials_df[param], bins=30, alpha=0.7)\n",
    "                    ax.axvline(summary[\"best_params\"][param], color='red', linestyle='dashed', \n",
    "                              linewidth=2, label=f'Best Value: {summary[\"best_params\"][param]}')\n",
    "                    ax.legend()\n",
    "                else:\n",
    "                    # Bar chart for categorical parameters\n",
    "                    value_counts = summary[\"parameter_stats\"][param][\"value_counts\"]\n",
    "                    categories = list(value_counts.keys())\n",
    "                    counts = list(value_counts.values())\n",
    "                    \n",
    "                    bars = ax.bar(categories, counts)\n",
    "                    \n",
    "                    # Highlight best value\n",
    "                    best_value = summary[\"best_params\"][param]\n",
    "                    if best_value in categories:\n",
    "                        best_idx = categories.index(best_value)\n",
    "                        bars[best_idx].set_color('red')\n",
    "                \n",
    "                ax.set_title(f'{param} Distribution')\n",
    "                ax.set_xlabel('Value')\n",
    "                ax.set_ylabel('Count')\n",
    "            \n",
    "            fig.tight_layout()\n",
    "            param_dist_img = fig_to_base64(fig)\n",
    "        else:\n",
    "            param_dist_img = None\n",
    "    except Exception as e:\n",
    "        param_dist_img = None\n",
    "        print(f\"Could not generate parameter distribution plot: {e}\")\n",
    "    \n",
    "    # Create the HTML content\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Optuna Hyperparameter Optimization Report - {summary[\"study_name\"]}</title>\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                line-height: 1.6;\n",
    "                color: #333;\n",
    "                max-width: 1200px;\n",
    "                margin: 0 auto;\n",
    "                padding: 20px;\n",
    "            }}\n",
    "            h1, h2, h3 {{\n",
    "                color: #2c3e50;\n",
    "            }}\n",
    "            h1 {{\n",
    "                border-bottom: 2px solid #3498db;\n",
    "                padding-bottom: 10px;\n",
    "            }}\n",
    "            h2 {{\n",
    "                border-bottom: 1px solid #bdc3c7;\n",
    "                padding-bottom: 5px;\n",
    "                margin-top: 30px;\n",
    "            }}\n",
    "            table {{\n",
    "                border-collapse: collapse;\n",
    "                width: 100%;\n",
    "                margin: 20px 0;\n",
    "            }}\n",
    "            th, td {{\n",
    "                text-align: left;\n",
    "                padding: 12px;\n",
    "                border-bottom: 1px solid #ddd;\n",
    "            }}\n",
    "            th {{\n",
    "                background-color: #f2f2f2;\n",
    "            }}\n",
    "            tr:hover {{\n",
    "                background-color: #f5f5f5;\n",
    "            }}\n",
    "            .highlight {{\n",
    "                background-color: #ffffcc;\n",
    "            }}\n",
    "            .visualization {{\n",
    "                margin: 30px 0;\n",
    "                text-align: center;\n",
    "            }}\n",
    "            .visualization img {{\n",
    "                max-width: 100%;\n",
    "                height: auto;\n",
    "                box-shadow: 0 4px 8px rgba(0,0,0,0.1);\n",
    "            }}\n",
    "            .summary-box {{\n",
    "                background-color: #f8f9fa;\n",
    "                border-left: 4px solid #3498db;\n",
    "                padding: 15px;\n",
    "                margin: 20px 0;\n",
    "            }}\n",
    "            .param-value {{\n",
    "                font-family: monospace;\n",
    "                background-color: #f0f0f0;\n",
    "                padding: 2px 4px;\n",
    "                border-radius: 3px;\n",
    "            }}\n",
    "            footer {{\n",
    "                margin-top: 50px;\n",
    "                padding-top: 20px;\n",
    "                border-top: 1px solid #eee;\n",
    "                text-align: center;\n",
    "                font-size: 0.9em;\n",
    "                color: #7f8c8d;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Optuna Hyperparameter Optimization Report</h1>\n",
    "        \n",
    "        <div class=\"summary-box\">\n",
    "            <h2>Study Summary</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Study Name</th>\n",
    "                    <td>{summary[\"study_name\"]}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Direction</th>\n",
    "                    <td>{summary[\"direction\"]}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Total Trials</th>\n",
    "                    <td>{summary[\"total_trials\"]}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Completed Trials</th>\n",
    "                    <td>{summary[\"completed_trials\"]}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Duration</th>\n",
    "                    <td>{summary[\"total_duration_hours\"]:.2f} hours</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Best Value</th>\n",
    "                    <td class=\"highlight\">{summary[\"best_value\"]:.6f}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Best Trial Number</th>\n",
    "                    <td>{summary[\"best_trial_number\"]}</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Best Parameters</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Parameter</th>\n",
    "                <th>Value</th>\n",
    "                <th>Type</th>\n",
    "                <th>Range/Values</th>\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add parameters to the table\n",
    "    for param, value in summary[\"best_params\"].items():\n",
    "        if param in summary[\"parameter_stats\"]:\n",
    "            param_stat = summary[\"parameter_stats\"][param]\n",
    "            \n",
    "            if param_stat[\"type\"] == \"numerical\":\n",
    "                range_str = f\"{param_stat['min']} to {param_stat['max']}\"\n",
    "            else:\n",
    "                # For categorical, show the possible values\n",
    "                categories = list(param_stat[\"value_counts\"].keys())\n",
    "                range_str = \", \".join(str(c) for c in categories[:5])\n",
    "                if len(categories) > 5:\n",
    "                    range_str += \", ...\"\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{param}</td>\n",
    "                <td class=\"param-value highlight\">{value}</td>\n",
    "                <td>{param_stat[\"type\"]}</td>\n",
    "                <td>{range_str}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </table>\n",
    "        \n",
    "        <h2>Visualizations</h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add visualizations\n",
    "    if optimization_history_img:\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"visualization\">\n",
    "            <h3>Optimization History</h3>\n",
    "            <img src=\"data:image/png;base64,{optimization_history_img}\" alt=\"Optimization History\">\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    if param_importance_img:\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"visualization\">\n",
    "            <h3>Parameter Importance</h3>\n",
    "            <img src=\"data:image/png;base64,{param_importance_img}\" alt=\"Parameter Importance\">\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    if param_dist_img:\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"visualization\">\n",
    "            <h3>Parameter Distributions</h3>\n",
    "            <img src=\"data:image/png;base64,{param_dist_img}\" alt=\"Parameter Distributions\">\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Top 10 trials\n",
    "    html_content += \"\"\"\n",
    "        <h2>Top 10 Trials</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Rank</th>\n",
    "                <th>Trial</th>\n",
    "                <th>Value</th>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add parameter columns\n",
    "    for param in summary[\"best_params\"].keys():\n",
    "        html_content += f\"<th>{param}</th>\\n\"\n",
    "    \n",
    "    html_content += \"</tr>\\n\"\n",
    "    \n",
    "    # Add trial rows\n",
    "    for i, trial in enumerate(summary[\"top_trials\"]):\n",
    "        html_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{i+1}</td>\n",
    "                <td>{trial[\"trial_number\"]}</td>\n",
    "                <td>{trial[\"value\"]:.6f}</td>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add parameter values\n",
    "        for param in summary[\"best_params\"].keys():\n",
    "            value = trial[\"params\"].get(param, \"N/A\")\n",
    "            html_content += f\"<td>{value}</td>\\n\"\n",
    "        \n",
    "        html_content += \"</tr>\\n\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </table>\n",
    "        \n",
    "        <h2>Parameter Statistics</h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add parameter statistics tables\n",
    "    # Separate numerical and categorical parameters\n",
    "    numerical_params = []\n",
    "    categorical_params = []\n",
    "    \n",
    "    for param, stats in summary[\"parameter_stats\"].items():\n",
    "        if stats[\"type\"] == \"numerical\":\n",
    "            numerical_params.append(param)\n",
    "        else:\n",
    "            categorical_params.append(param)\n",
    "    \n",
    "    # Numerical parameters table\n",
    "    if numerical_params:\n",
    "        html_content += \"\"\"\n",
    "        <h3>Numerical Parameters</h3>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Parameter</th>\n",
    "                <th>Min</th>\n",
    "                <th>Max</th>\n",
    "                <th>Mean</th>\n",
    "                <th>Median</th>\n",
    "                <th>Std Dev</th>\n",
    "                <th>Best Value</th>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        for param in numerical_params:\n",
    "            stats = summary[\"parameter_stats\"][param]\n",
    "            html_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{param}</td>\n",
    "                <td>{stats[\"min\"]}</td>\n",
    "                <td>{stats[\"max\"]}</td>\n",
    "                <td>{stats[\"mean\"]:.4f}</td>\n",
    "                <td>{stats[\"median\"]}</td>\n",
    "                <td>{stats[\"std\"]:.4f}</td>\n",
    "                <td class=\"highlight\">{stats[\"best_value\"]}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"</table>\\n\"\n",
    "    \n",
    "    # Categorical parameters\n",
    "    if categorical_params:\n",
    "        html_content += \"\"\"\n",
    "        <h3>Categorical Parameters</h3>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Parameter</th>\n",
    "                <th>Most Common Value</th>\n",
    "                <th>Count</th>\n",
    "                <th>Best Value</th>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        for param in categorical_params:\n",
    "            stats = summary[\"parameter_stats\"][param]\n",
    "            most_common = stats[\"most_common\"]\n",
    "            count = stats[\"value_counts\"][most_common]\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{param}</td>\n",
    "                <td>{most_common}</td>\n",
    "                <td>{count}</td>\n",
    "                <td class=\"highlight\">{stats[\"best_value\"]}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"</table>\\n\"\n",
    "    \n",
    "    # Parameter importance table\n",
    "    if \"parameter_importance\" in summary and isinstance(summary[\"parameter_importance\"], dict) and \"error\" not in summary[\"parameter_importance\"]:\n",
    "        html_content += \"\"\"\n",
    "        <h3>Parameter Importance</h3>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Rank</th>\n",
    "                <th>Parameter</th>\n",
    "                <th>Importance Score</th>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sort parameters by importance\n",
    "        sorted_importance = sorted(summary[\"parameter_importance\"].items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i, (param, score) in enumerate(sorted_importance):\n",
    "            html_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{i+1}</td>\n",
    "                <td>{param}</td>\n",
    "                <td>{score:.4f}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"</table>\\n\"\n",
    "    \n",
    "    # Footer\n",
    "    html_content += f\"\"\"\n",
    "        <footer>\n",
    "            <p>Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} using Optuna {optuna.__version__}</p>\n",
    "        </footer>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Export results to various formats\n",
    "def export_study_results(study, trials_df, summary, formats=None):\n",
    "    \"\"\"Export study results to various formats.\"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    from IPython.display import IFrame\n",
    "    import numpy as np\n",
    "    \n",
    "    if formats is None:\n",
    "        formats = [\"csv\", \"json\", \"html\"]\n",
    "    \n",
    "    export_path = \"./optuna_results\"\n",
    "    os.makedirs(export_path, exist_ok=True)\n",
    "    \n",
    "    exported_files = {}\n",
    "    \n",
    "    # 1. Export trials to CSV\n",
    "    if \"csv\" in formats:\n",
    "        csv_path = os.path.join(export_path, \"trials.csv\")\n",
    "        trials_df.to_csv(csv_path, index=False)\n",
    "        exported_files[\"csv\"] = csv_path\n",
    "        print(f\"Exported trials to CSV: {csv_path}\")\n",
    "    \n",
    "    # 2. Export summary to JSON\n",
    "    if \"json\" in formats:\n",
    "        json_path = os.path.join(export_path, \"summary.json\")\n",
    "        \n",
    "        # Helper function to convert NumPy types to Python types\n",
    "        def json_serializable(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, (np.bool_, bool)):\n",
    "                return bool(obj)\n",
    "            elif isinstance(obj, (dict, list)):\n",
    "                return sanitize_for_json(obj)\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        # Recursively sanitize dictionaries and lists\n",
    "        def sanitize_for_json(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                return {key: json_serializable(value) for key, value in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [json_serializable(item) for item in obj]\n",
    "            else:\n",
    "                return json_serializable(obj)\n",
    "        \n",
    "        # Convert the summary to a JSON-serializable format\n",
    "        json_friendly_summary = sanitize_for_json(summary)\n",
    "        \n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(json_friendly_summary, f, indent=2)\n",
    "        \n",
    "        exported_files[\"json\"] = json_path\n",
    "        print(f\"Exported summary to JSON: {json_path}\")\n",
    "    \n",
    "    # 3. Export report to HTML\n",
    "    if \"html\" in formats:\n",
    "        html_path = os.path.join(export_path, \"report.html\")\n",
    "        html_content = create_html_report(study, trials_df, summary)\n",
    "        \n",
    "        with open(html_path, \"w\") as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        exported_files[\"html\"] = html_path\n",
    "        print(f\"Exported report to HTML: {html_path}\")\n",
    "        \n",
    "        # Display the HTML report in a frame\n",
    "        display(IFrame(html_path, width=\"100%\", height=600))\n",
    "    \n",
    "    return exported_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the study summary\n",
    "print(\"## Generating Study Summary\")\n",
    "study_summary = generate_study_summary(study, trials_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of the best trial\n",
    "print(\"\\nBest Trial Summary:\")\n",
    "print(f\"Trial Number: {study_summary['best_trial_number']}\")\n",
    "print(f\"Objective Value: {study_summary['best_value']:.6f}\")\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in study_summary['best_params'].items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display brief performance stats\n",
    "print(\"\\nPerformance Statistics:\")\n",
    "ps = study_summary['performance_stats']\n",
    "print(f\"  Min Value: {ps['min_value']:.6f}\")\n",
    "print(f\"  Max Value: {ps['max_value']:.6f}\")\n",
    "print(f\"  Mean Value: {ps['mean_value']:.6f}\")\n",
    "print(f\"  Median Value: {ps['median_value']:.6f}\")\n",
    "print(f\"  Standard Deviation: {ps['std_value']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "print(\"\\n## Exporting Results\")\n",
    "print(\"Exporting study results to CSV, JSON, and HTML formats...\")\n",
    "exported_files = export_study_results(study, trials_df, study_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the exported files\n",
    "print(\"\\nExported files:\")\n",
    "for fmt, path in exported_files.items():\n",
    "    print(f\"  {fmt.upper()}: {path}\")\n",
    "\n",
    "print(\"\\nThe HTML report provides a comprehensive overview of the hyperparameter optimization results.\")\n",
    "print(\"You can share this report with others to communicate your findings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusions and Future Directions\n",
    "\n",
    "# Display a summary of the key findings\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Function to format markdown content\n",
    "def display_md(md_content):\n",
    "    display(Markdown(md_content))\n",
    "\n",
    "# Create a summary markdown\n",
    "md_content = \"\"\"\n",
    "## Key Findings\n",
    "\n",
    "### Optimization Performance\n",
    "- **Total Trials:** {total_trials} completed out of {all_trials} total trials\n",
    "- **Best Value:** {best_value:.6f} (achieved at trial #{best_trial})\n",
    "- **Value Range:** {min_value:.6f} to {max_value:.6f} (mean: {mean_value:.6f})\n",
    "- **Optimization Time:** {duration:.2f} hours\n",
    "\n",
    "### Key Hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "# Get data for the markdown\n",
    "total_trials = len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE]))\n",
    "all_trials = len(study.trials)\n",
    "best_value = study.best_value\n",
    "best_trial = study.best_trial.number\n",
    "min_value = trials_df['value'].min()\n",
    "max_value = trials_df['value'].max()\n",
    "mean_value = trials_df['value'].mean()\n",
    "\n",
    "# Calculate study duration\n",
    "if hasattr(study.best_trial, 'datetime_start') and study.best_trial.datetime_start is not None:\n",
    "    start_times = [t.datetime_start for t in study.trials if t.datetime_start is not None]\n",
    "    end_times = [t.datetime_complete for t in study.trials if t.datetime_complete is not None]\n",
    "    \n",
    "    if start_times and end_times:\n",
    "        duration = (max(end_times) - min(start_times)).total_seconds() / 3600  # Convert to hours\n",
    "    else:\n",
    "        duration = 0\n",
    "else:\n",
    "    duration = 0\n",
    "\n",
    "# Format the markdown with actual values\n",
    "md_content = md_content.format(\n",
    "    total_trials=total_trials,\n",
    "    all_trials=all_trials,\n",
    "    best_value=best_value,\n",
    "    best_trial=best_trial,\n",
    "    min_value=min_value,\n",
    "    max_value=max_value,\n",
    "    mean_value=mean_value,\n",
    "    duration=duration\n",
    ")\n",
    "\n",
    "# Add key hyperparameters information\n",
    "try:\n",
    "    # Get parameter importance if available\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    top_params = list(importance.items())[:5]  # Top 5 parameters\n",
    "    \n",
    "    for param, score in top_params:\n",
    "        md_content += f\"- **{param}:** {importance[param]:.4f} importance ({study.best_params[param]})\\n\"\n",
    "except:\n",
    "    # If importance calculation fails, just list parameters\n",
    "    for param, value in study.best_params.items():\n",
    "        md_content += f\"- **{param}:** {value}\\n\"\n",
    "\n",
    "# Add observations and insights\n",
    "md_content += \"\"\"\n",
    "## Observations and Insights\n",
    "\n",
    "### What Worked Well\n",
    "- The optimization process successfully identified a configuration that achieved good performance.\n",
    "- The search space exploration appears to have been effective, with a diverse set of trials.\n",
    "- We can see clear patterns in which hyperparameters had the most impact on model performance.\n",
    "\n",
    "### Areas for Improvement\n",
    "- Some hyperparameters may benefit from further refinement with a narrower search space.\n",
    "- Based on the learning curve, we might not have fully converged to the optimal solution.\n",
    "- Consider additional hyperparameters that were not included in this study.\n",
    "\n",
    "## Future Directions\n",
    "\n",
    "### Next Steps for Hyperparameter Optimization\n",
    "1. **Focused Search:** Run a new study with narrowed parameter ranges around the best configurations.\n",
    "2. **Alternative Algorithms:** Try different sampling algorithms (e.g., CMA-ES, TPE, or Random) to compare results.\n",
    "3. **Early Pruning:** Implement more aggressive pruning to save computation time on unpromising trials.\n",
    "4. **Additional Parameters:** Incorporate other hyperparameters that might affect model performance.\n",
    "\n",
    "### Model Deployment Considerations\n",
    "1. **Model Selection:** Choose between the top-performing configurations based on additional criteria (speed, memory usage).\n",
    "2. **Ensemble Methods:** Consider creating an ensemble of top models for potentially better performance.\n",
    "3. **Cross-Validation:** Validate the best configuration with cross-validation for more robust performance estimates.\n",
    "4. **Model Explainability:** Analyze how the best hyperparameters affect model predictions and behavior.\n",
    "\n",
    "### Further Analysis Ideas\n",
    "1. **Interaction Effects:** Explore interactions between hyperparameters and their joint effects on performance.\n",
    "2. **Feature Engineering:** Use insights from hyperparameter importance to guide feature engineering efforts.\n",
    "3. **Comparative Studies:** Compare these results with other model architectures or frameworks.\n",
    "4. **Sensitivity Analysis:** Test the robustness of the best configuration to small perturbations in hyperparameters.\n",
    "\"\"\"\n",
    "\n",
    "# Display the markdown content\n",
    "display_md(md_content)\n",
    "\n",
    "# Create a visual summarizing the optimization journey\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Optimization history with best value\n",
    "ax1 = axes[0, 0]\n",
    "trials_data = [(t.number, t.value) for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "if trials_data:\n",
    "    numbers, values = zip(*trials_data)\n",
    "    \n",
    "    # Calculate best value at each point\n",
    "    best_values = []\n",
    "    best_so_far = float('inf')\n",
    "    for v in values:\n",
    "        if v < best_so_far:\n",
    "            best_so_far = v\n",
    "        best_values.append(best_so_far)\n",
    "    \n",
    "    # Plot trials\n",
    "    ax1.scatter(numbers, values, alpha=0.5, label='Trials')\n",
    "    ax1.plot(numbers, best_values, 'r-', label='Best Value')\n",
    "    \n",
    "    # Formatting\n",
    "    ax1.set_xlabel('Trial Number')\n",
    "    ax1.set_ylabel('Objective Value')\n",
    "    ax1.set_title('Optimization History')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# 2. Top parameter values\n",
    "ax2 = axes[0, 1]\n",
    "try:\n",
    "    # Get top 2 parameters by importance\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    top_2_params = list(importance.keys())[:2]\n",
    "    \n",
    "    if len(top_2_params) >= 2:\n",
    "        param1, param2 = top_2_params\n",
    "        \n",
    "        # Check if parameters are numeric\n",
    "        if all(trials_df[p].dtype in [np.float64, np.int64] for p in [param1, param2]):\n",
    "            # Create scatter plot\n",
    "            sc = ax2.scatter(\n",
    "                trials_df[param1], \n",
    "                trials_df[param2], \n",
    "                c=trials_df['value'], \n",
    "                cmap='viridis', \n",
    "                alpha=0.7\n",
    "            )\n",
    "            \n",
    "            # Highlight best trial\n",
    "            best_idx = trials_df['value'].idxmin()\n",
    "            ax2.scatter(\n",
    "                trials_df.loc[best_idx, param1],\n",
    "                trials_df.loc[best_idx, param2],\n",
    "                c='red',\n",
    "                marker='*',\n",
    "                s=200,\n",
    "                label='Best'\n",
    "            )\n",
    "            \n",
    "            # Formatting\n",
    "            ax2.set_xlabel(param1)\n",
    "            ax2.set_ylabel(param2)\n",
    "            ax2.set_title(f'Top Parameters: {param1} vs {param2}')\n",
    "            ax2.grid(True, linestyle='--', alpha=0.7)\n",
    "            ax2.legend()\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(sc, ax=ax2)\n",
    "            cbar.set_label('Objective Value')\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'Cannot plot non-numeric parameters', \n",
    "                    ha='center', va='center', transform=ax2.transAxes)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Not enough parameters for scatter plot', \n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "except Exception as e:\n",
    "    ax2.text(0.5, 0.5, f'Error creating parameter plot: {str(e)}', \n",
    "            ha='center', va='center', transform=ax2.transAxes)\n",
    "\n",
    "# 3. Performance distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(trials_df['value'], bins=30, alpha=0.7, color='skyblue')\n",
    "ax3.axvline(best_value, color='red', linestyle='dashed', linewidth=2, \n",
    "           label=f'Best Value: {best_value:.6f}')\n",
    "ax3.set_xlabel('Objective Value')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Distribution of Objective Values')\n",
    "ax3.grid(True, linestyle='--', alpha=0.7)\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Parameter importance bar chart\n",
    "ax4 = axes[1, 1]\n",
    "try:\n",
    "    importance = optuna.importance.get_param_importances(study)\n",
    "    params = list(importance.keys())\n",
    "    scores = list(importance.values())\n",
    "    \n",
    "    # Limit to top 10 parameters\n",
    "    if len(params) > 10:\n",
    "        params = params[:10]\n",
    "        scores = scores[:10]\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    y_pos = np.arange(len(params))\n",
    "    ax4.barh(y_pos, scores, align='center', color='navy', alpha=0.7)\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels(params)\n",
    "    ax4.invert_yaxis()  # Labels read top-to-bottom\n",
    "    ax4.set_xlabel('Importance Score')\n",
    "    ax4.set_title('Parameter Importance')\n",
    "    ax4.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax4.grid(True, linestyle='--', alpha=0.7, axis='x')\n",
    "except Exception as e:\n",
    "    ax4.text(0.5, 0.5, f'Error creating importance plot: {str(e)}', \n",
    "            ha='center', va='center', transform=ax4.transAxes)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final message\n",
    "print(\"\\nThis concludes the analysis of the Optuna hyperparameter tuning results.\")\n",
    "print(\"The generated visualizations and reports provide comprehensive insights into the optimization process.\")\n",
    "print(\"\\nRecommendations for future work:\")\n",
    "print(\"1. Focus on the most important parameters identified in this analysis\")\n",
    "print(\"2. Consider running additional trials with a narrowed parameter space around the best configuration\")\n",
    "print(\"3. Explore ensemble methods using multiple top-performing configurations\")\n",
    "print(\"4. Validate the best models with cross-validation to ensure robustness\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
