experiment:
  run_name: aoifemac_flasc
  log_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/logging/
  project_root: /Users/ahenry/Documents/toolboxes/wind_forecasting

# logging:
#   wandb_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/logging/ # WandB will create a 'wandb' subdirectory here automatically
#   optuna_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/logging/optuna/
#   checkpoint_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/logging/checkpoints/
#   slurm_output_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/logging/slurm_logs/

optuna:
  storage_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/optuna
  n_trials: 2
  metric: mean_wQuantileLoss
  direction: minimize
  max_epochs: 2
  limit_train_batches: 5
  
  # Pruning configuration
  pruning:
    enabled: true             # Whether to enable pruning
    type: "hyperband"         # Options: "hyperband", "median", "percentile", "none"
    min_resource: 1           # Minimum epochs before pruning can occur (default: 2)
    reduction_factor: 3       # Hyperband reduction factor (default: 3)
    max_resource: 5
    percentile": 25
  
  # Optuna visualization configuration (additional to wandb)
  visualization:
    enabled: true
    output_dir: ${logging.optuna_dir}/visualizations
    plots:
      optimization_history: true
      parameter_importance: true
      slice_plot: true
      
  # Storage backend configuration
  storage:
    # backend: "sqlite"  # Options: "postgresql", "sqlite", "mysql", "journal"
    # # --- SQLite Specific Settings (not used if backend is postgresql) ---
    # sqlite_wal: true  # Enable WAL mode for SQLite
    # sqlite_timeout: 600 # Timeout in seconds for SQLite locks

    # "backend": "journal"

    "backend": "mysql"

    # --- Optional TCP/IP Settings (if use_socket is false) ---
    # use_tcp: false
    # db_host: "localhost"
    # db_port: 5432

    # --- Optional Command Execution Settings ---
    # run_cmd_shell: false # Set to true if specific commands require shell=True


  # Optuna Dashboard auto-launch configuration
  dashboard:
    enabled: true             # Set to true to automatically launch the dashboard on rank 0
    port: 8088                # Port for the dashboard web server
    log_file: "${logging.optuna_dir}/optuna_dashboard.log" # Log file for the dashboard process

dataset: 
    data_path: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data/preprocessed_flasc_data/SMARTEOLE_WakeSteering_SCADA_1minData_normalized.parquet
    normalization_consts_path: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/data/preprocessed_flasc_data/SMARTEOLE_WakeSteering_SCADA_1minData_normalization_consts.csv
    context_length: 600 # in seconds
    prediction_length: 300 # in seconds
    target_turbine_ids: # or leave blank to capture all
    normalize: False 
    batch_size: 128
    workers: 12
    overfit: False
    test_split: 0.20
    val_split: 0.10
    resample_freq: 60s
    n_splits: 1 # how many divisions of each continuity group to make which is further subdivided into training test and validation data
    per_turbine_target: False 
    context_length_factor: 2

model:
  distr_output: 
    class: LowRankMultivariateNormalOutput
    kwargs:
      rank: 8
  informer:
    # embedding_dimension: 32 # Determines dimension of the embedding space
    num_encoder_layers: 2 # Number of transformer blocks stacked
    num_decoder_layers: 1 # Number of transformer blocks stacked
    n_heads: 8 # Number of heads for spatio-temporal attention
    d_model: 512
    dim_feedforward: 2048
    activation: relu
  autoformer:
    # embedding_dimension: 32 # Determines dimension of the embedding space
    num_encoder_layers: 2 # Number of transformer blocks stacked
    num_decoder_layers: 1 # Number of transformer blocks stacked
    n_heads: 8 # Number of heads for spatio-temporal attention
    dim_feedforward: 2048
    activation: gelu
  spacetimeformer:
    # embedding_dimension: 32 # Determines dimension of the embedding space
    num_encoder_layers: 3 # Number of transformer blocks stacked
    num_decoder_layers: 3 # Number of transformer blocks stacked
    n_heads: 4 # Number of heads for spatio-temporal attention
    d_model: 200
    dim_feedforward: 800
    d_queries_keys: 30
    d_values: 30
    dropout_emb: 0.2
    dropout_attn_matrix: 0.0
    dropout_attn_out: 0.0
    dropout_ff: 0.3
    dropout_qkv: 0.0
    start_token_len: 0
    performer_redraw_interval: 100
    use_shifted_time_windows: False
    # decay_factor: 0.25
    # l2_coeff: 1e-6
    # class_loss_imp: 0.1
    pos_emb_type: abs
    embed_method: spatio-temporal
    activation: gelu
    use_given: False
    
callbacks: 
    progress_bar:  
    early_stopping:  
    model_checkpoint:  
    lr_monitor: True

trainer: 
    # grad_clip_norm: 0.0 # Prevents gradient explosion if > 0 
    # limit_val_batches: 1.0 
    # val_check_interval: 1.0
    accelerator: cpu
    devices: auto
    num_nodes: 1
    strategy: auto
    # n_workers: auto
    # debug: False 
    # accumulate: 1.0
    max_epochs: 10 # Maximum number of epochs to train 100
    limit_train_batches: 1000
    default_root_dir: /Users/ahenry/Documents/toolboxes/wind_forecasting/examples/logging/
    # precision: 32-true # 16-mixed enables mixed precision training 32-true is full precision
    # batch_size: 32 # larger = more stable gradients
    # lr: 0.0001 # Step size
    # dropout: 0.1 # Regularization parameter (prevents overfitting)
    # patience: 50 # Number of epochs to wait before early stopping
    # accumulate_grad_batches: 2 # Simulates a larger batch size
